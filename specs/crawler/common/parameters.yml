CrawlerIdParameter:
  name: id
  in: path
  description: Crawler ID.
  required: true
  schema:
    $ref: '#/CrawlerID'

TaskIdParameter:
  name: taskID
  in: path
  description: Task ID.
  required: true
  schema:
    $ref: '#/TaskID'

CrawlerVersionParameter:
  name: version
  in: path
  description: The version of the targeted Crawler revision.
  required: true
  schema:
    type: integer

ItemsPerPage:
  name: itemsPerPage
  in: query
  description: Number of items per page to retrieve.
  schema:
    $ref: '#/itemsPerPage'

Page:
  name: page
  in: query
  description: Page to retrieve.
  schema:
    $ref: '#/page'

Name:
  name: name
  in: query
  description: Name of the crawler for filtering the API response.
  schema:
    $ref: '#/CrawlerName'

AppID:
  name: appID
  in: query
  description: Algolia application ID for filtering the API response.
  schema:
    $ref: '#/applicationID'

applicationID:
  type: string
  description: |
    Algolia application ID where the crawler creates and updates indices.
    The Crawler add-on must be enabled for this application.

CrawlerID:
  type: string
  description: Universally unique identifier (UUID) of the crawler.
  example: e0f6db8a-24f5-4092-83a4-1b2c6cb6d809

TaskID:
  type: string
  description: Universally unique identifier (UUID) of the task.
  example: 98458796-b7bb-4703-8b1b-785c1080b110

CrawlerName:
  type: string
  maxLength: 64
  description: Name of the crawler.
  example: test-crawler

UrlsCrawledGroup:
  type: object
  description: Processed URLs and their status.
  properties:
    status:
      $ref: '#/urlsCrawledGroupStatus'
    reason:
      type: string
      description: Reason for this status.
    category:
      $ref: '#/urlsCrawledGroupCategory'
    count:
      type: integer
      description: Number of URLs with this status.
    readable:
      type: string
      description: Readable representation of the reason for the status message.
  example:
    status: SKIPPED
    reason: forbidden_by_robotstxt
    category: fetch
    count: 3
    readable: Forbidden by robots.txt

urlsCrawledGroupStatus:
  type: string
  description: Status of crawling these URLs.
  enum:
    - DONE
    - SKIPPED
    - FAILED

urlsCrawledGroupCategory:
  type: string
  description: Step where the status information was generated.
  enum:
    - fetch
    - extraction
    - indexing
    - success

itemsPerPage:
  type: integer
  description: Number of items per page of the paginated API response.
  minimum: 1
  maximum: 100
  default: 20

page:
  type: integer
  description: Current page of the paginated API response.
  minimum: 1
  maximum: 100
  default: 1

total:
  type: integer
  description: Total number of retrievable items.
  example: 100

Pagination:
  type: object
  description: Pagination information.
  properties:
    itemsPerPage:
      $ref: '#/itemsPerPage'
    page:
      $ref: '#/page'
    total:
      $ref: '#/total'

version:
  type: integer
  description: Version of the configuration. Version 1 is the initial configuration you used when creating the crawler.
  minimum: 1

authorId:
  type: string
  description: Universally unique identifier (UUID) of the user who created this version of the configuration.
  example: 7d79f0dd-2dab-4296-8098-957a1fdc0637
