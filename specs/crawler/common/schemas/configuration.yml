Configuration:
  type: object
  description: Crawler configuration.
  required:
    - appId
    - rateLimit
    - actions
  properties:
    actions:
      type: array
      description: A list of actions.
      minItems: 1
      maxItems: 30
      items:
        $ref: './action.yml#/Action'
    apiKey:
      type: string
      description: |
        Algolia API key for indexing the records.

        For more information, see the [`apiKey` documentation](https://www.algolia.com/doc/tools/crawler/apis/apikey/).
    appId:
      $ref: '../parameters.yml#/applicationID'
    exclusionPatterns:
      type: array
      description: URLs to exclude from crawling.
      maxItems: 100
      example:
        - https://www.example.com/excluded
        - '!https://www.example.com/this-one-url'
        - https://www.example.com/exclude/**
      items:
        type: string
        description: |
          URLs to exclude from crawling.

          Uses [micromatch](https://github.com/micromatch/micromatch) for negation, wildcards, and more.
    externalData:
      type: array
      description: |
        References to external data sources for enriching the extracted records.

        For more information, see [Enrich extracted records with external data](https://www.algolia.com/doc/tools/crawler/guides/enriching-extraction-with-external-data/).
      maxItems: 10
      items:
        type: string
        description: Reference to an external data source you configured in the Crawler dashboard.
        example: testCSV
    extraUrls:
      type: array
      maxItems: 9999
      description: |
        The Crawler treats `extraUrls` the same as `startUrls`.
        Specify `extraUrls` if you want to differentiate between URLs you manually added to fix site crawling from those you initially specified in `startUrls`.
      items:
        type: string
    ignoreCanonicalTo:
      $ref: '#/ignoreCanonicalTo'
    ignoreNoFollowTo:
      type: boolean
      description: |
        Whether to ignore the `nofollow` meta tag or link attribute.

        For more information, see the [`ignoreNoFollowTo` documentation](https://www.algolia.com/doc/tools/crawler/apis/ignorenofollowto/).
    ignoreNoIndex:
      type: boolean
      description: |
        Whether to ignore the `noindex` robots meta tag.
        If `true` pages with this meta tag _will_ be crawled.
    ignoreQueryParams:
      type: array
      description: |
        Query parameters to ignore while crawling.

        All URLs with the matching query parameters will be treated as identical.
        This prevents indexing URLs that just differ by their query parameters.

        You can use wildcard characters to pattern match.
      maxItems: 9999
      example:
        - ref
        - utm_*
      items:
        type: string
        description: Query parameter to ignore. You can include wildcards to match a range of similar query parameters.
    ignoreRobotsTxtRules:
      type: boolean
      description: Whether to ignore rules defined in your `robots.txt` file.
    indexPrefix:
      type: string
      description: A prefix for all indices created by this crawler. It's combined with the `indexName` for each action to form the complete index name.
      maxLength: 64
      example: crawler_
    initialIndexSettings:
      type: object
      description: |
        Crawler index settings.

        These index settings are only applied during the first crawl of an index.
        Any subsequent changes won't be applied to the index.
        Instead, make changes to your index settings in the [Algolia dashboard](https://dashboard.algolia.com/explorer/configuration/).
      additionalProperties:
        $ref: '../../../common/schemas/IndexSettings.yml#/indexSettings'
        x-additionalPropertiesName: indexName
    linkExtractor:
      title: linkExtractor
      type: object
      description: |
        Function for extracting URLs from links on crawled pages.

        For more information, see the [`linkExtractor` documentation](https://www.algolia.com/doc/tools/crawler/apis/linkextractor/).
      properties:
        __type:
          $ref: './action.yml#/configurationRecordExtractorType'
        source:
          type: string
          example: |
            ({ $, url, defaultExtractor }) => {
              if (/example.com\/doc\//.test(url.href)) {
                // For all pages under `/doc`, only extract the first found URL.
                return defaultExtractor().slice(0, 1)
              }
              // For all other pages, use the default.
              return defaultExtractor()
            }
    login:
      $ref: '#/login'
    maxDepth:
      type: number
      description: |
        Maximum path depth of crawled URLs.
        For example, if `maxDepth` is 2, `https://example.com/foo/bar` is crawled,
        but `https://example.com/foo/bar/baz` won't.
        Trailing slashes increase the URL depth.
      minimum: 1
      maximum: 100
    maxUrls:
      type: number
      description: |
        Limits the number of URLs your crawler processes.

        Change it to a low value, such as 100, for quick crawling tests.
        Change it to a higher explicit value for full crawls to prevent it from getting "lost" in complex site structures.

        Because the Crawler works on many pages simultaneously, `maxUrls` doesn't guarantee finding the same pages each time it runs.
      minimum: 1
      maximum: 15000000
    rateLimit:
      type: number
      description: |
        Number of concurrent tasks per second.

        If processing each URL takes _n_ seconds,
        your crawler can process `rateLimit / n` URLs per second.

        Higher numbers mean faster crawls but they also increase your bandwidth and server load.
      minimum: 1
      maximum: 100
      example: 4
    renderJavaScript:
      $ref: '#/renderJavaScript'
    requestOptions:
      $ref: '#/requestOptions'
    safetyChecks:
      $ref: '#/safetyChecks'
    saveBackup:
      type: boolean
      description: |
        Whether to back up your index before the crawler overwrites it with new records.
    schedule:
      $ref: '#/schedule'
    sitemaps:
      type: array
      description: Sitemaps with URLs from where to start crawling.
      maxItems: 9999
      items:
        type: string
        example: https://example.com/sitemap.xyz
    startUrls:
      type: array
      description: URLs from where to start crawling.
      maxItems: 9999
      items:
        type: string
        example: https://www.example.com

PartialConfig:
  description: |
    Crawler configuration to update.
    You can only update top-level configuration properties.
    To update a nested configuration, such as `actions.recordExtractor`,
    you must provide the complete top-level object such as `actions`.
  allOf:
    - $ref: '#/Configuration'

ignoreCanonicalTo:
  oneOf:
    - type: boolean
      description: |
        Determines if the crawler should extract records from a page with a [canonical URL](https://www.algolia.com/doc/tools/crawler/getting-started/crawler-configuration/#canonical-urls-and-crawler-behaviorr).

        If ignoreCanonicalTo is set to:

        - `true` all canonical URLs are ignored.
        - One or more URL patterns, the crawler will ignore the canonical URL if it matches a pattern.
    - type: array
      description: |
        Canonical URLs or URL patterns to ignore.
      items:
        type: string
        description: |
          Pattern or URL.

          Canonical URLs are only ignored if they match this pattern.

renderJavaScript:
  description: |
    If `true`, use a Chrome headless browser to crawl pages.

    Because crawling JavaScript-based web pages is slower than crawling regular HTML pages, you can apply this setting to a specific list of pages. 
    Use [micromatch](https://github.com/micromatch/micromatch) to define URL patterns, including negations and wildcards.
  oneOf:
    - type: boolean
      description: Whether to render all pages.
    - type: array
      description: URLs or URL patterns to render.
      items:
        type: string
        description: URL or URL pattern to render.
        example:
          - http://www.mysite.com/dynamic-pages/**
    - title: headlessBrowserConfig
      type: object
      description: Configuration for rendering HTML.
      properties:
        enabled:
          type: boolean
          description: Whether to enable JavaScript rendering.
          example: true
        patterns:
          type: array
          description: URLs or URL patterns to render.
          items:
            type: string
          example:
            - http://www.mysite.com/dynamic-pages/**
        adBlock:
          type: boolean
          default: false
          description: |
            Whether to use the Crawler's ad blocker.
            It blocks most ads and tracking scripts but can break some sites.
        waitTime:
          $ref: '#/waitTime'
      required:
        - enabled
        - patterns

requestOptions:
  type: object
  description: Lets you add options to HTTP requests made by the crawler.
  properties:
    proxy:
      type: string
      description: Proxy for all crawler requests.
    timeout:
      type: number
      default: 30000
      description: Timeout in milliseconds for the crawl.
    retries:
      type: number
      default: 3
      description: Maximum number of retries to crawl one URL.
    headers:
      $ref: '#/headers'

waitTime:
  type: object
  description: Timeout for the HTTP request.
  properties:
    min:
      type: number
      default: 0
      description: Minimum waiting time in milliseconds.
      example: 7000
    max:
      type: number
      default: 20000
      description: Maximum waiting time in milliseconds.
      example: 15000

initialIndexSettings:
  type: object
  description: |
    [Index settings](/specs/search#tag/_model_index_settings).

headers:
  type: object
  description: Headers to add to all requests.
  properties:
    Accept-Language:
      type: string
      description: Preferred natural language and locale.
      example: 'fr-FR'
    Authorization:
      type: string
      description: Basic authentication header.
      example: 'Bearer Aerehdf=='
    Cookie:
      type: string
      description: Cookie. The header will be replaced by the cookie retrieved when logging in.
      example: session=1234

login:
  description: Authorization method and credentials for crawling protected content.
  oneOf:
    - $ref: '#/fetchRequest'
    - $ref: '#/browserRequest'
    - $ref: '#/oauthRequest'

fetchRequest:
  type: object
  description: Information for making a HTTP request for authorization.
  properties:
    url:
      type: string
      description: URL with your login form.
      example: https://example.com/login
    requestOptions:
      $ref: '#/loginRequestOptions'
  required:
    - url

browserRequest:
  type: object
  description: |
    Information for using a web browser for authorization.
    The browser loads a login page and enters the provided credentials.
  properties:
    url:
      type: string
      description: |
        URL of your login page.

        The crawler looks for an input matching the selector `input[type=text]` or `input[type=email]` for the username and `input[type=password]` for the password.
      example: https://example.com/login
    username:
      type: string
      description: Username for signing in.
      example: crawler
    password:
      type: string
      description: Password for signing in.
      example: s3cr3t
    waitTime:
      $ref: '#/waitTime'
  required:
    - url
    - username
    - password

oauthRequest:
  type: object
  description: |
    Authorization information for using the [OAuth 2.0 client credentials](https://datatracker.ietf.org/doc/html/rfc6749#section-4.4) authorization grant.
    OAuth authorization is supported for [Azure Active Directory version 1](https://learn.microsoft.com/en-us/previous-versions/azure/active-directory/azuread-dev/v1-oauth2-client-creds-grant-flow) as provider.
  properties:
    accessTokenRequest:
      $ref: '#/accessTokenRequest'
  required:
    - accessTokenRequest

loginRequestOptions:
  type: object
  description: Options for the HTTP request for logging in.
  properties:
    method:
      type: string
      description: HTTP method for sending the request.
      default: GET
      example: POST
    headers:
      $ref: '#/headers'
    body:
      type: string
      description: Form content.
      example: 'id=user&password=s3cr3t'
    timeout:
      type: number
      description: Timeout for the request.

accessTokenRequest:
  type: object
  description: |
    Parameters required to make the [access token request](https://datatracker.ietf.org/doc/html/rfc6749#section-4.4.2).
  properties:
    url:
      type: string
      description: URL for the access token endpoint.
    grantType:
      $ref: '#/grantType'
    clientId:
      type: string
      description: |
        [Client identifier](https://datatracker.ietf.org/doc/html/rfc6749#section-2.2).
    clientSecret:
      type: string
      description: Client secret.
    scope:
      type: string
      description: |
        [Access token scope](https://datatracker.ietf.org/doc/html/rfc6749#section-3.3).
    extraParameters:
      $ref: '#/extraParameters'
  required:
    - url
    - grantType
    - clientId
    - clientSecret

grantType:
  type: string
  description: OAuth 2.0 grant type.
  enum:
    - client_credentials

extraParameters:
  type: object
  description: Extra parameters for the authorization request.
  properties:
    resource:
      type: string
      description: |
        App ID URI of the receiving web service.

        For more information, see [Azure Active Directory](https://learn.microsoft.com/en-us/previous-versions/azure/active-directory/azuread-dev/v1-oauth2-client-creds-grant-flow#first-case-access-token-request-with-a-shared-secret).

safetyChecks:
  type: object
  description: |
    Checks to ensure the crawl was successful.

    For more information, see the [Safety checks](https://www.algolia.com/doc/tools/crawler/getting-started/crawler-configuration/#safety-checks) documentation.
  properties:
    beforeIndexPublishing:
      $ref: '#/beforeIndexPublishing'

beforeIndexPublishing:
  type: object
  description: Checks triggered after the crawl finishes but before the records are added to the Algolia index.
  properties:
    maxLostRecordsPercentage:
      type: number
      description: Maximum difference in percent between the numbers of records between crawls.
      minimum: 1
      maximum: 100
      default: 10
    maxFailedUrls:
      type: number
      description: Stops the crawler if a specified number of pages fail to crawl.

schedule:
  type: string
  description: |
    Schedule for running the crawl.

    For more information, see the [`schedule` documentation](https://www.algolia.com/doc/tools/crawler/apis/schedule/).
  example: every weekday at 12:00 pm
