Configuration:
  type: object
  description: Crawler configuration.
  required:
    - appId
    - rateLimit
    - actions
  properties:
    actions:
      type: array
      description: A list of actions.
      minItems: 1
      maxItems: 30
      items:
        $ref: './action.yml#/Action'
    apiKey:
      type: string
      description: |
        Algolia API key for indexing the records.

        For more information, see the [`apiKey` documentation](https://www.algolia.com/doc/tools/crawler/apis/configuration/api-key/).
    appId:
      $ref: '../parameters.yml#/applicationID'
    exclusionPatterns:
      type: array
      description: URLs to exclude from crawling.
      maxItems: 100
      example:
        - https://www.example.com/excluded
        - '!https://www.example.com/this-one-url'
        - https://www.example.com/exclude/**
      items:
        type: string
        description: |
          URLs to exclude from crawling.

          Uses [micromatch](https://github.com/micromatch/micromatch) for negation, wildcards, and more.
    externalData:
      type: array
      description: |
        References to external data sources for enriching the extracted records.

        For more information, see [Enrich extracted records with external data](https://www.algolia.com/doc/tools/crawler/guides/enriching-extraction-with-external-data/).
      maxItems: 10
      items:
        type: string
        description: Reference to an external data source you configured in the Crawler dashboard.
        example: testCSV
    extraUrls:
      type: array
      maxItems: 9999
      description: |
        URLs from where to start crawling.

        For more information, see the [`extraUrls` documentation](https://www.algolia.com/doc/tools/crawler/apis/configuration/extra-urls/).
      items:
        type: string
    ignoreCanonicalTo:
      $ref: '#/ignoreCanonicalTo'
    ignoreNoFollowTo:
      type: boolean
      description: |
        Whether to ignore the `nofollow` meta tag or link attribute.

        For more information, see the [`ignoreNoFollowTo` documentation](https://www.algolia.com/doc/tools/crawler/apis/configuration/ignore-no-follow-to/).
    ignoreNoIndex:
      type: boolean
      description: |
        Whether to ignore the `noindex` robots meta tag.
        If `true` pages with this meta tag _will_ be crawled.
    ignoreQueryParams:
      type: array
      description: |
        Query parameters to ignore while crawling.

        All URLs with the matching query parameters will be treated as identical.
        This prevents indexing URLs that just differ by their query parameters.

        You can use wildcard characters to pattern match.
      maxItems: 9999
      example:
        - ref
        - utm_*
      items:
        type: string
        description: Query parameter to ignore. You can include wildcards to match a range of similar query parameters.
    ignoreRobotsTxtRules:
      type: boolean
      description: Whether to ignore rules defined in your `robots.txt` file.
    indexPrefix:
      type: string
      description: A prefix for all indices created by this crawler. It's combined with the `indexName` for each action to form the complete index name.
      maxLength: 64
      example: crawler_
    initialIndexSettings:
      type: object
      description: |
        Crawler index settings.

        For more information, see the [`initialIndexSettings` documentation](https://www.algolia.com/doc/tools/crawler/apis/configuration/initial-index-settings/).
      additionalProperties:
        $ref: '../../../common/schemas/IndexSettings.yml#/indexSettings'
        x-additionalPropertiesName: indexName
    linkExtractor:
      title: linkExtractor
      type: object
      description: |
        Function for extracting URLs from links on crawled pages.

        For more information, see the [`linkExtractor` documentation](https://www.algolia.com/doc/tools/crawler/apis/configuration/link-extractor/).
      properties:
        __type:
          $ref: './action.yml#/configurationRecordExtractorType'
        source:
          type: string
          example: |
            ({ $, url, defaultExtractor }) => {
              if (/example.com\/doc\//.test(url.href)) {
                // For all pages under `/doc`, only extract the first found URL.
                return defaultExtractor().slice(0, 1)
              }
              // For all other pages, use the default.
              return defaultExtractor()
            }
    login:
      $ref: '#/login'
    maxDepth:
      type: number
      description: |
        Maximum path depth of crawled URLs.
        For example, if `maxDepth` is 2, `https://example.com/foo/bar` is crawled,
        but `https://example.com/foo/bar/baz` won't.
        Trailing slashes increase the URL depth.
      minimum: 1
      maximum: 100
    maxUrls:
      type: number
      description: |
        Maximum number of crawled URLs.

        Setting `maxUrls` doesn't guarantee consistency between crawls
        because the crawler processes URLs in parallel.
      minimum: 1
      maximum: 15000000
    rateLimit:
      type: number
      description: |
        Number of concurrent tasks per second.

        If processing each URL takes _n_ seconds,
        your crawler can process `rateLimit / n` URLs per second.

        Higher numbers mean faster crawls but they also increase your bandwidth and server load.
      minimum: 1
      maximum: 100
      example: 4
    renderJavaScript:
      $ref: '#/renderJavaScript'
    requestOptions:
      $ref: '#/requestOptions'
    safetyChecks:
      $ref: '#/safetyChecks'
    saveBackup:
      type: boolean
      description: |
        Whether to back up your index before the crawler overwrites it with new records.
    schedule:
      $ref: '#/schedule'
    sitemaps:
      type: array
      description: Sitemaps with URLs from where to start crawling.
      maxItems: 9999
      items:
        type: string
        example: https://example.com/sitemap.xyz
    startUrls:
      type: array
      description: URLs from where to start crawling.
      maxItems: 9999
      items:
        type: string
        example: https://www.example.com

PartialConfig:
  description: |
    Crawler configuration to update.
    You can only update top-level configuration properties.
    To update a nested configuration, such as `actions.recordExtractor`,
    you must provide the complete top-level object such as `actions`.
  allOf:
    - $ref: '#/Configuration'

ignoreCanonicalTo:
  oneOf:
    - type: boolean
      description: |
        Whether to ignore canonical redirects.

        If true, canonical URLs for pages are ignored.
    - type: array
      description: |
        Canonical URLs or URL patterns to ignore.
      items:
        type: string
        description: |
          Pattern or URL.

          Canonical URLs are only ignored if they match this pattern.

renderJavaScript:
  description: |
    Crawl JavaScript-rendered pages with a headless browser.

    For more information, see the [`renderJavaScript` documentation](https://www.algolia.com/doc/tools/crawler/apis/configuration/render-java-script/).
  oneOf:
    - type: boolean
      description: Whether to render all pages.
    - type: array
      description: URLs or URL patterns to render.
      items:
        type: string
        description: URL or URL pattern to render.
        example: https://www.example.com
    - title: headlessBrowserConfig
      type: object
      description: Configuration for rendering HTML.
      properties:
        enabled:
          type: boolean
          description: Whether to render matching URLs.
        patterns:
          type: array
          description: URLs or URL patterns to render.
          items:
            type: string
        adBlock:
          type: boolean
          description: |
            Whether to turn on the built-in adblocker.
            This blocks most ads and tracking scripts but can break some sites.
        waitTime:
          $ref: '#/waitTime'
      required:
        - enabled
        - patterns

requestOptions:
  type: object
  description: Options to add to all HTTP requests made by the crawler.
  properties:
    proxy:
      type: string
      description: Proxy for all crawler requests.
    timeout:
      type: number
      default: 30000
      description: Timeout in milliseconds for the crawl.
    retries:
      type: number
      default: 3
      description: Maximum number of retries to crawl one URL.
    headers:
      $ref: '#/headers'

waitTime:
  type: object
  description: Timeout for the HTTP request.
  properties:
    min:
      type: number
      default: 0
      description: Minimum waiting time in milliseconds.
    max:
      type: number
      default: 20000
      description: Maximum waiting time in milliseconds.

initialIndexSettings:
  type: object
  description: |
    [Index settings](/specs/search#tag/_model_index_settings).

headers:
  type: object
  description: Headers to add to all requests.
  properties:
    Accept-Language:
      type: string
      description: Preferred natural language and locale.
      example: 'fr-FR'
    Authorization:
      type: string
      description: Basic authentication header.
      example: 'Bearer Aerehdf=='
    Cookie:
      type: string
      description: Cookie. The header will be replaced by the cookie retrieved when logging in.
      example: session=1234

login:
  description: Authorization method and credentials for crawling protected content.
  oneOf:
    - $ref: '#/fetchRequest'
    - $ref: '#/browserRequest'
    - $ref: '#/oauthRequest'

fetchRequest:
  type: object
  description: Information for making a HTTP request for authorization.
  properties:
    url:
      type: string
      description: URL with your login form.
      example: https://example.com/login
    requestOptions:
      $ref: '#/loginRequestOptions'
  required:
    - url

browserRequest:
  type: object
  description: |
    Information for using a web browser for authorization.
    The browser loads a login page and enters the provided credentials.
  properties:
    url:
      type: string
      description: |
        URL of your login page.

        The crawler looks for an input matching the selector `input[type=text]` or `input[type=email]` for the username and `input[type=password]` for the password.
      example: https://example.com/login
    username:
      type: string
      description: Username for signing in.
      example: crawler
    password:
      type: string
      description: Password for signing in.
      example: s3cr3t
    waitTime:
      $ref: '#/waitTime'
  required:
    - url
    - username
    - password

oauthRequest:
  type: object
  description: |
    Authorization information for using the [OAuth 2.0 client credentials](https://datatracker.ietf.org/doc/html/rfc6749#section-4.4) authorization grant.
    OAuth authorization is supported for [Azure Active Directory version 1](https://learn.microsoft.com/en-us/previous-versions/azure/active-directory/azuread-dev/v1-oauth2-client-creds-grant-flow) as provider.
  properties:
    accessTokenRequest:
      $ref: '#/accessTokenRequest'
  required:
    - accessTokenRequest

loginRequestOptions:
  type: object
  description: Options for the HTTP request for logging in.
  properties:
    method:
      type: string
      description: HTTP method for sending the request.
      default: GET
      example: POST
    headers:
      $ref: '#/headers'
    body:
      type: string
      description: Form content.
      example: 'id=user&password=s3cr3t'
    timeout:
      type: number
      description: Timeout for the request.

accessTokenRequest:
  type: object
  description: |
    Parameters required to make the [access token request](https://datatracker.ietf.org/doc/html/rfc6749#section-4.4.2).
  properties:
    url:
      type: string
      description: URL for the access token endpoint.
    grantType:
      $ref: '#/grantType'
    clientId:
      type: string
      description: |
        [Client identifier](https://datatracker.ietf.org/doc/html/rfc6749#section-2.2).
    clientSecret:
      type: string
      description: Client secret.
    scope:
      type: string
      description: |
        [Access token scope](https://datatracker.ietf.org/doc/html/rfc6749#section-3.3).
    extraParameters:
      $ref: '#/extraParameters'
  required:
    - url
    - grantType
    - clientId
    - clientSecret

grantType:
  type: string
  description: OAuth 2.0 grant type.
  enum:
    - client_credentials

extraParameters:
  type: object
  description: Extra parameters for the authorization request.
  properties:
    resource:
      type: string
      description: |
        App ID URI of the receiving web service.

        For more information, see [Azure Active Directory](https://learn.microsoft.com/en-us/previous-versions/azure/active-directory/azuread-dev/v1-oauth2-client-creds-grant-flow#first-case-access-token-request-with-a-shared-secret).

safetyChecks:
  type: object
  description: |
    Checks to ensure the crawl was successful.

    For more information, see the [Safety checks](https://www.algolia.com/doc/tools/crawler/getting-started/crawler-configuration/#safety-checks) documentation.
  properties:
    beforeIndexPublishing:
      $ref: '#/beforeIndexPublishing'

beforeIndexPublishing:
  type: object
  description: Checks triggered after the crawl finishes but before the records are added to the Algolia index.
  properties:
    maxLostRecordsPercentage:
      type: number
      description: Maximum difference in percent between the numbers of records between crawls.
      minimum: 1
      maximum: 100
      default: 10
    maxFailedUrls:
      type: number
      description: Stops the crawler if a specified number of pages fail to crawl.

schedule:
  type: string
  description: |
    Schedule for running the crawl.

    For more information, see the [`schedule` documentation](https://www.algolia.com/doc/tools/crawler/apis/configuration/schedule/).
  example: every weekday at 12:00 pm
