Configuration:
  type: object
  description: Crawler configuration.
  required:
    - appId
    - rateLimit
    - actions
  properties:
    actions:
      type: array
      description: A list of actions.
      minItems: 1
      maxItems: 30
      items:
        $ref: './action.yml#/Action'
    apiKey:
      type: string
      description: |
        The Algolia API key the crawler uses for indexing records.
        If you don't provide an API key, one will be generated by the Crawler when you create a configuration.

        The API key must have:

        - These [rights and restrictions](https://www.algolia.com/doc/guides/security/api-keys/#rights-and-restrictions): `search`, `addObject`, `deleteObject`, `deleteIndex`, `settings`, `editSettings`, `listIndexes`, `browse`
        - Access to the correct set of indices, based on the crawler's `indexPrefix`.
        For example, if the prefix is `crawler_`, the API key must have access to `crawler_*`.

        **Don't use your [Admin API key](https://www.algolia.com/doc/guides/security/api-keys/#predefined-api-keys)**.
    appId:
      $ref: '../parameters.yml#/applicationID'
    exclusionPatterns:
      type: array
      description: URLs to exclude from crawling.
      maxItems: 100
      example:
        - https://www.example.com/excluded
        - '!https://www.example.com/this-one-url'
        - https://www.example.com/exclude/**
      items:
        type: string
        description: |
          Use [micromatch](https://github.com/micromatch/micromatch) for negation, wildcards, and more.
    externalData:
      type: array
      description: |
        References to external data sources for enriching the extracted records.
      maxItems: 10
      items:
        type: string
        description: For more information, see [Enrich extracted records with external data](https://www.algolia.com/doc/tools/crawler/guides/enriching-extraction-with-external-data).
        example: testCSV
    extraUrls:
      type: array
      maxItems: 9999
      description: |
        The Crawler treats `extraUrls` the same as `startUrls`.
        Specify `extraUrls` if you want to differentiate between URLs you manually added to fix site crawling from those you initially specified in `startUrls`.
      items:
        type: string
    ignoreCanonicalTo:
      $ref: '#/ignoreCanonicalTo'
    ignoreNoFollowTo:
      type: boolean
      description: |
        Determines if the crawler should follow links with a `nofollow` directive.
        If `true`, the crawler will ignore the `nofollow` directive and crawl links on the page.

        The crawler always ignores links that don't match your [configuration settings](https://www.algolia.com/doc/tools/crawler/getting-started/crawler-configuration/#exclude-and-include-content).
        `ignoreNoFollowTo` applies to:

        - Links that are ignored because the [`robots` meta tag](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/meta/name#Other_metadata_names) contains `nofollow` or `none`.
        - Links with a [`rel` attribute](https://developer.mozilla.org/en-US/docs/Web/HTML/Attributes/rel) containing the `nofollow` directive.
    ignoreNoIndex:
      type: boolean
      description: |
        Whether to ignore the `noindex` robots meta tag.
        If `true`, pages with this meta tag _will_ be crawled.
    ignorePaginationAttributes:
      type: boolean
      description: |
        Whether the crawler should follow `rel="prev"` and `rel="next"` pagination links in the `<head>` section of an HTML page.

        - If `true`, the crawler ignores the pagination links.
        - If `false`, the crawler follows the pagination links.
      default: true
    ignoreQueryParams:
      type: array
      description: |
        Query parameters to ignore while crawling.

        All URLs with the matching query parameters are treated as identical.
        This prevents indexing URLs that just differ by their query parameters.
      maxItems: 9999
      example:
        - ref
        - utm_*
      items:
        type: string
        description: Use wildcards to match multiple query parameters.
    ignoreRobotsTxtRules:
      type: boolean
      description: Whether to ignore rules defined in your `robots.txt` file.
    indexPrefix:
      type: string
      description: A prefix for all indices created by this crawler. It's combined with the `indexName` for each action to form the complete index name.
      maxLength: 64
      example: crawler_
    initialIndexSettings:
      type: object
      description: |
        Crawler index settings.

        These index settings are only applied during the first crawl of an index.

        Any subsequent changes won't be applied to the index.
        Instead, make changes to your index settings in the [Algolia dashboard](https://dashboard.algolia.com/explorer/configuration).
      additionalProperties:
        $ref: '../../../common/schemas/IndexSettings.yml#/indexSettings'
        x-additionalPropertiesName: indexName
    linkExtractor:
      title: linkExtractor
      type: object
      description: |
        Function for extracting URLs from links on crawled pages.

        For more information, see the [`linkExtractor` documentation](https://www.algolia.com/doc/tools/crawler/apis/configuration/link-extractor).
      properties:
        __type:
          $ref: './action.yml#/configurationRecordExtractorType'
        source:
          type: string
          example: |
            ({ $, url, defaultExtractor }) => {
              if (/example.com\/doc\//.test(url.href)) {
                // For all pages under `/doc`, only extract the first found URL.
                return defaultExtractor().slice(0, 1)
              }
              // For all other pages, use the default.
              return defaultExtractor()
            }
    login:
      $ref: '#/login'
    maxDepth:
      type: integer
      description: |
        Determines the maximum path depth of crawled URLs.

        Path depth is calculated based on the number of slash characters (`/`) after the domain (starting at 1).
        For example:

        - **1** `http://example.com`
        - **1** `http://example.com/`
        - **1** `http://example.com/foo`
        - **2** `http://example.com/foo/`
        - **2** `http://example.com/foo/bar`
        - **3** `http://example.com/foo/bar/`

        **URLs added with `startUrls` and `sitemaps` aren't checked for `maxDepth`.**.
      minimum: 1
      maximum: 100
      example: 5
    maxUrls:
      type: integer
      description: |
        Limits the number of URLs your crawler processes.

        Change it to a low value, such as 100, for quick crawling tests.
        Change it to a higher explicit value for full crawls to prevent it from getting "lost" in complex site structures.
        Because the Crawler works on many pages simultaneously, `maxUrls` doesn't guarantee finding the same pages each time it runs.
      minimum: 1
      maximum: 15000000
      example: 250
    rateLimit:
      type: integer
      description: |
        Determines the number of concurrent tasks per second that can run for this configuration.

        A higher rate limit means more crawls per second.
        Algolia prevents system overload by ensuring the number of URLs added in the last second and the number of URLs being processed is less than the rate limit:


        ```
        max(new_urls_added, active_urls_processing) <= rateLimit
        ```

        Start with a low value (for example, 2) and increase it if you need faster crawling.
        Be aware that a high `rateLimit` can have a huge impact on bandwidth cost and server resource consumption.

        The number of pages processed per second depends on the average time it takes to fetch, process, and upload a URL. 
        For a given `rateLimit` if fetching, processing, and uploading URLs takes (on average):

        - Less than a second, your crawler processes up to `rateLimit` pages per second.
        - Four seconds, your crawler processes up to `rateLimit / 4` pages per second.

        In the latter case, increasing `rateLimit` improves performance, up to a point. 
        However, if the processing time remains at four seconds, increasing `rateLimit` won't increase the number of pages processed per second.
      minimum: 1
      maximum: 100
      example: 4
    renderJavaScript:
      $ref: '#/renderJavaScript'
    requestOptions:
      $ref: '#/requestOptions'
    safetyChecks:
      $ref: '#/safetyChecks'
    saveBackup:
      type: boolean
      description: |
        Whether to back up your index before the crawler overwrites it with new records.
    schedule:
      $ref: '#/schedule'
    sitemaps:
      type: array
      description: Sitemaps with URLs from where to start crawling.
      maxItems: 9999
      items:
        type: string
        example: https://example.com/sitemap.xyz
    startUrls:
      type: array
      description: URLs from where to start crawling.
      maxItems: 9999
      items:
        type: string
        example: https://www.example.com

PartialConfig:
  description: |
    Crawler configuration to update.
    You can only update top-level configuration properties.
    To update a nested configuration, such as `actions.recordExtractor`,
    you must provide the complete top-level object such as `actions`.
  allOf:
    - $ref: '#/Configuration'

ignoreCanonicalTo:
  oneOf:
    - type: boolean
      description: |
        Determines if the crawler should extract records from a page with a [canonical URL](https://www.algolia.com/doc/tools/crawler/getting-started/crawler-configuration/#canonical-urls-and-crawler-behavior).

        If `ignoreCanonicalTo` is set to:

        - `true` all canonical URLs are ignored.
        - One or more URL patterns, the crawler will ignore the canonical URL if it matches a pattern.
    - type: array
      description: |
        Canonical URLs or URL patterns to ignore.
      items:
        type: string
        description: |
          Pattern or URL.

          Canonical URLs are only ignored if they match this pattern.

renderJavaScript:
  description: |
    If `true`, use a Chrome headless browser to crawl pages.

    Because crawling JavaScript-based web pages is slower than crawling regular HTML pages, you can apply this setting to a specific list of pages. 
    Use [micromatch](https://github.com/micromatch/micromatch) to define URL patterns, including negations and wildcards.
  oneOf:
    - type: boolean
      description: Whether to render all pages.
    - type: array
      description: URLs or URL patterns to render.
      items:
        type: string
        description: URL or URL pattern to render.
        example:
          - http://www.mysite.com/dynamic-pages/**
    - title: headlessBrowserConfig
      type: object
      description: Configuration for rendering HTML.
      properties:
        enabled:
          type: boolean
          description: Whether to enable JavaScript rendering.
          example: true
        patterns:
          type: array
          description: URLs or URL patterns to render.
          items:
            type: string
          example:
            - http://www.mysite.com/dynamic-pages/**
        adBlock:
          type: boolean
          default: false
          description: |
            Whether to use the Crawler's ad blocker.
            It blocks most ads and tracking scripts but can break some sites.
        waitTime:
          $ref: '#/waitTime'
      required:
        - enabled
        - patterns

requestOptions:
  type: object
  description: Lets you add options to HTTP requests made by the crawler.
  properties:
    proxy:
      type: string
      description: Proxy for all crawler requests.
    timeout:
      type: integer
      default: 30000
      description: Timeout in milliseconds for the crawl.
    retries:
      type: integer
      default: 3
      description: Maximum number of retries to crawl one URL.
    headers:
      $ref: '#/headers'

waitTime:
  type: object
  description: Timeout for the HTTP request.
  properties:
    min:
      type: integer
      default: 0
      description: Minimum waiting time in milliseconds.
      example: 7000
    max:
      type: integer
      default: 20000
      description: Maximum waiting time in milliseconds.
      example: 15000

initialIndexSettings:
  type: object
  description: |
    [Index settings](/specs/search#tag/_model_index_settings).

headers:
  type: object
  description: Headers to add to all requests.
  properties:
    Accept-Language:
      type: string
      description: Preferred natural language and locale.
      example: 'fr-FR'
    Authorization:
      type: string
      description: Basic authentication header.
      example: 'Bearer Aerehdf=='
    Cookie:
      type: string
      description: Cookie. The header will be replaced by the cookie retrieved when logging in.
      example: session=1234

login:
  description: |
    Authorization method and credentials for crawling protected content.

    The Crawler supports these authentication methods:

    - **Basic authentication**.
      The Crawler obtains a session cookie from the login page.
    - **OAuth 2.0 authentication** (`oauthRequest`).
      The Crawler uses OAuth 2.0 client credentials to obtain an access token for authentication.

    **Basic authentication**

    The Crawler extracts the `Set-Cookie` response header from the login page, stores that cookie,
    and sends it in the `Cookie` header when crawling all pages defined in the configuration.

    This cookie is retrieved only at the start of each full crawl.
    If it expires, it isn't automatically renewed.

    The Crawler can obtain the session cookie in one of two ways:

    - **HTTP request authentication** (`fetchRequest`).
      The Crawler sends a direct request with your credentials to the login endpoint, similar to a `curl` command.
    - **Browser-based authentication** (`browserRequest`).
      The Crawler emulates a web browser by loading the login page, entering the credentials,
      and submitting the login form as a real user would.

    **OAuth 2.0**

    The crawler supports [OAuth 2.0 client credentials grant flow](https://datatracker.ietf.org/doc/html/rfc6749#section-4.4):

    1. It performs an access token request with the provided credentials
    1. Stores the fetched token in an `Authorization` header
    1. Sends the token when crawling site pages.

    This token is only fetched at the beginning of each complete crawl.
    If it expires, it isn't automatically renewed.

    Client authentication passes the credentials (`client_id` and `client_secret`) [in the request body](https://datatracker.ietf.org/doc/html/rfc6749#section-2.3.1).
    The [Azure AD v1.0](https://learn.microsoft.com/en-us/previous-versions/azure/active-directory/azuread-dev/v1-oauth2-client-creds-grant-flow) provider is supported.
  oneOf:
    - $ref: '#/fetchRequest'
    - $ref: '#/browserRequest'
    - $ref: '#/oauthRequest'

fetchRequest:
  title: HTTP request
  type: object
  description: Information for making a HTTP request for authorization.
  properties:
    url:
      type: string
      description: URL with your login form.
      example: https://example.com/login
    requestOptions:
      $ref: '#/loginRequestOptions'
  required:
    - url
  example:
    url: 'https://example.com/secure/login-with-post'
    requestOptions:
      method: 'POST'
      headers:
        Content-Type: 'application/x-www-form-urlencoded'
      body: 'id=my-id&password=my-password'
      timeout: 5000

browserRequest:
  title: Browser-based
  type: object
  description: |
    Information for using a web browser for authorization.
    The browser loads a login page and enters the provided credentials.
  properties:
    url:
      type: string
      description: |
        URL of your login page.

        The crawler looks for an input matching the selector `input[type=text]` or `input[type=email]` for the username and `input[type=password]` for the password.
      example: https://example.com/login
    username:
      type: string
      description: Username for signing in.
      example: crawler
    password:
      type: string
      description: Password for signing in.
      example: s3cr3t
    waitTime:
      $ref: '#/waitTime'
  required:
    - url
    - username
    - password
  example:
    url: 'https://example.com/secure/login-page'
    username: 'my-id'
    password: 'my-password'

oauthRequest:
  title: OAuth 2.0
  type: object
  description: |
    Authorization information for using the [OAuth 2.0 client credentials](https://datatracker.ietf.org/doc/html/rfc6749#section-4.4) authorization grant.

    OAuth authorization is supported for [Azure Active Directory version 1](https://learn.microsoft.com/en-us/previous-versions/azure/active-directory/azuread-dev/v1-oauth2-client-creds-grant-flow) as provider.
  properties:
    accessTokenRequest:
      $ref: '#/accessTokenRequest'
  required:
    - accessTokenRequest
  example:
    accessTokenRequest:
      url: 'https://example.com/oauth2/token'
      grant_type: 'client_credentials'
      client_id: 'my-client-id'
      client_secret: 'my-client-secret'
      extraParameters:
        resource: 'https://protected.example.com/'

loginRequestOptions:
  type: object
  description: Options for the HTTP request for logging in.
  properties:
    method:
      type: string
      description: HTTP method for sending the request.
      default: GET
      example: POST
    headers:
      $ref: '#/headers'
    body:
      type: string
      description: Form content.
      example: 'id=user&password=s3cr3t'
    timeout:
      type: integer
      description: Timeout for the request.

accessTokenRequest:
  type: object
  description: |
    Parameters required to make the [access token request](https://datatracker.ietf.org/doc/html/rfc6749#section-4.4.2).
  properties:
    url:
      type: string
      description: URL for the access token endpoint.
    grantType:
      $ref: '#/grantType'
    clientId:
      type: string
      description: |
        [Client identifier](https://datatracker.ietf.org/doc/html/rfc6749#section-2.2).
    clientSecret:
      type: string
      description: Client secret.
    scope:
      type: string
      description: |
        [Access token scope](https://datatracker.ietf.org/doc/html/rfc6749#section-3.3).
    extraParameters:
      $ref: '#/extraParameters'
  required:
    - url
    - grantType
    - clientId
    - clientSecret

grantType:
  type: string
  description: OAuth 2.0 grant type.
  enum:
    - client_credentials

extraParameters:
  type: object
  description: Extra parameters for the authorization request.
  properties:
    resource:
      type: string
      description: |
        App ID URI of the receiving web service.

        For more information, see [Azure Active Directory](https://learn.microsoft.com/en-us/previous-versions/azure/active-directory/azuread-dev/v1-oauth2-client-creds-grant-flow#first-case-access-token-request-with-a-shared-secret).
safetyChecks:
  type: object
  description: |
    Checks to ensure the crawl was successful.

    For more information, see the [Safety checks](https://www.algolia.com/doc/tools/crawler/getting-started/crawler-configuration/#safety-checks) documentation.
  properties:
    beforeIndexPublishing:
      $ref: '#/beforeIndexPublishing'

beforeIndexPublishing:
  type: object
  description: Checks triggered after the crawl finishes but before the records are added to the Algolia index.
  properties:
    maxLostRecordsPercentage:
      type: integer
      description: Maximum difference in percent between the numbers of records between crawls.
      minimum: 1
      maximum: 100
      default: 10
    maxFailedUrls:
      type: integer
      description: Stops the crawler if a specified number of pages fail to crawl.

schedule:
  type: string
  description: |
    Schedule for running the crawl.

    Instead of manually starting a crawl each time, you can set up a schedule for automatic crawls.
    [Use the visual UI](https://www.algolia.com/doc/tools/crawler/getting-started/crawler-configuration-visual) or add the `schedule` parameter to [your configuration](https://www.algolia.com/doc/tools/crawler/getting-started/crawler-configuration).

    `schedule` uses [Later.js syntax](https://bunkat.github.io/later) to specify when to crawl your site.
    Here are some key things to keep in mind when using `Later.js` syntax with the Crawler:

    - The interval between two scheduled crawls must be at least 24 hours.
    - To crawl daily, use "every 1 day" instead of "everyday" or "every day".
    - If you don't specify a time, the crawl can happen any time during the scheduled day.
    - Specify times for the UTC (GMT+0) timezone
    - Include minutes when specifying a time. For example, "at 3:00 pm" instead of "at 3pm".
    - Use "at 12:00 am" to specify midnight, not "at 00:00 am".
  example: every weekday at 12:00 pm
