Configuration:
  type: object
  description: Crawler configuration.
  required:
    - appId
    - rateLimit
    - actions
  properties:
    actions:
      type: array
      description: |
        Instructions about how to process crawled URLs.

        Each action defines:

        - The targeted subset of URLs it processes.
        - What information to extract from the web pages.
        - The Algolia indices where the extracted records will be stored.

        A single web page can match multiple actions.
        In this case, the crawler produces one record for each matched action.
      minItems: 1
      maxItems: 30
      items:
        $ref: './action.yml#/Action'
    apiKey:
      type: string
      description: |
        Algolia API key for indexing the records.

        The API key must have the following access control list (ACL) permissions:
        `search`, `browse`, `listIndexes`, `addObject`, `deleteObject`, `deleteIndex`, `settings`, `editSettings`.
        The API key must not be the admin API key of the application.
        The API key must have access to create the indices that the crawler will use.
        For example, if `indexPrefix` is `crawler_`, the API key must have access to all `crawler_*` indices.
    appId:
      $ref: '../parameters.yml#/applicationID'
    exclusionPatterns:
      type: array
      description: URLs to exclude from crawling.
      maxItems: 100
      example:
        - https://www.example.com/excluded
        - '!https://www.example.com/this-one-url'
        - https://www.example.com/exclude/**
      items:
        type: string
        description: |
          Pattern for matching URLs to exclude from crawling.

          The pattern support globs and wildcard matching with [micromark](https://github.com/micromatch/micromatch).
    externalData:
      type: array
      description: |
        References to external data sources for enriching the extracted records.

        For more information, see [Enrich extracted records with external data](https://www.algolia.com/doc/tools/crawler/guides/enriching-extraction-with-external-data/).
      maxItems: 10
      items:
        type: string
        description: Reference to an external data source you configured in the Crawler dashboard.
        example: testCSV
    extraUrls:
      type: array
      maxItems: 9999
      description: |
        URLs from where to start crawling.

        These are the same as `startUrls`.
        URLs you [crawl manually](#tag/actions/operation/testUrl) can be added to `extraUrls`.
      items:
        type: string
    ignoreCanonicalTo:
      $ref: '#/ignoreCanonicalTo'
    ignoreNoFollowTo:
      type: boolean
      description: |
        Whether to ignore the `nofollow` meta tag or link attribute.
        If true, links with the `rel="nofollow"` attribute or links on pages with the `nofollow` robots meta tag will be crawled.
    ignoreNoIndex:
      type: boolean
      description: |
        Whether to ignore the `noindex` robots meta tag.
        If true, pages with this meta tag will be crawled.
    ignoreQueryParams:
      type: array
      description: |
        Query parameters to ignore while crawling.

        All URLs with the matching query parameters will be treated as identical.
        This prevents indexing duplicated URLs, that just differ by their query parameters.
      maxItems: 9999
      example:
        - ref
        - utm_*
      items:
        type: string
        description: Query parameter to ignore. You can include wildcards to match a range of similar query parameters.
    ignoreRobotsTxtRules:
      type: boolean
      description: Whether to ignore rules defined in your `robots.txt` file.
    indexPrefix:
      type: string
      description: A prefix for all indices created by this crawler. It's combined with the `indexName` for each action to form the complete index name.
      maxLength: 64
      example: crawler_
    initialIndexSettings:
      type: object
      description: |
        Initial index settings, one settings object per index.

        This setting is only applied when the index is first created.
        Settings are not re-applied.
        This prevents overriding any settings changes after the index was created.
      additionalProperties:
        $ref: '../../../common/schemas/IndexSettings.yml#/indexSettings'
        x-additionalPropertiesName: indexName
    linkExtractor:
      title: linkExtractor
      type: object
      description: Function for extracting URLs for links found on crawled pages.
      properties:
        __type:
          $ref: './action.yml#/configurationRecordExtractorType'
        source:
          type: string
          description: |
            JavaScript function (as a string) for extracting URLs for links found on crawled pages.
            By default, all URLs that comply with the `pathsToMatch`, `fileTypesToMatch`, and `exclusions` settings are added to the crawl.
            The [Crawler dashboard](https://crawler.algolia.com/admin) has an editor with autocomplete and validation,
            which makes editing the `linkExtractor` property easier.
          example: |
            ({ $, url, defaultExtractor }) => {
              if (/example.com\/doc\//.test(url.href)) {
                // For all pages under `/doc`, only extract the first found URL.
                return defaultExtractor().slice(0, 1)
              }
              // For all other pages, use the default.
              return defaultExtractor()
            }
    login:
      $ref: '#/login'
    maxDepth:
      type: number
      description: |
        Maximum path depth of crawled URLs.
        For example, if `maxDepth` is 2, `https://example.com/foo/bar` is crawled,
        but `https://example.com/foo/bar/baz` won't.
        Trailing slashes increase the URL depth.
      minimum: 1
      maximum: 100
    maxUrls:
      type: number
      description: |
        Maximum number of crawled URLs.

        Setting `maxUrls` doesn't guarantee consistency between crawls
        because the crawler processes URLs in parallel.
      minimum: 1
      maximum: 15000000
    rateLimit:
      type: number
      description: |
        Number of concurrent tasks per second.

        If processing each URL takes _n_ seconds,
        your crawler can process `rateLimit / n` URLs per second.

        Higher numbers mean faster crawls but they also increase your bandwidth and server load.
      minimum: 1
      maximum: 100
      example: 4
    renderJavaScript:
      $ref: '#/renderJavaScript'
    requestOptions:
      $ref: '#/requestOptions'
    safetyChecks:
      $ref: '#/safetyChecks'
    saveBackup:
      type: boolean
      description: |
        Whether to back up your index before the crawler overwrites it with new records.
    schedule:
      $ref: '#/schedule'
    sitemaps:
      type: array
      description: Sitemaps with URLs from where to start crawling.
      maxItems: 9999
      items:
        type: string
        example: https://example.com/sitemap.xyz
    startUrls:
      type: array
      description: URLs from where to start crawling.
      maxItems: 9999
      items:
        type: string
        example: https://www.example.com

PartialConfig:
  description: |
    Crawler configuration to update.
    You can only update top-level configuration properties.
    To update a nested configuration, such as `actions.recordExtractor`,
    you must provide the complete top-level object such as `actions`.
  allOf:
    - $ref: '#/Configuration'

ignoreCanonicalTo:
  oneOf:
    - type: boolean
      description: |
        Whether to ignore canonical redirects.

        If true, canonical URLs for pages are ignored.
    - type: array
      description: |
        Canonical URLs or URL patterns to ignore.
      items:
        type: string
        description: |
          Pattern or URL.

          Canonical URLs are only ignored if they match this pattern.

renderJavaScript:
  description: |
    Crawl JavaScript-rendered pages by rendering them with a headless browser.

    Rendering JavaScript-based pages is slower than crawling regular HTML pages.
  oneOf:
    - type: boolean
      description: Whether to render all pages with a headless browser.
    - type: array
      description: URLs or patterns which to render with a headless browser.
      items:
        type: string
        description: |
          URL or pattern for matching URLs which to render with a headless browser.

          The pattern support globs and wildcard matching with [micromark](https://github.com/micromatch/micromatch).
        example: https://www.example.com
    - title: headlessBrowserConfig
      type: object
      description: Configuration for rendering HTML with a headless browser.
      properties:
        enabled:
          type: boolean
          description: Whether to render matching URLs with a headless browser.
        patterns:
          type: array
          description: |
            URLs or patterns for matching URLs that should be rendered with a headless browser.

            The pattern support globs and wildcard matching with [micromark](https://github.com/micromatch/micromatch).
          items:
            type: string
        adBlock:
          type: boolean
          description: |
            Whether to turn on the built-in adblocker.
            This blocks most ads and tracking scripts but can break some websites.
        waitTime:
          $ref: '#/waitTime'
      required:
        - enabled
        - patterns

requestOptions:
  type: object
  description: Options to add to all HTTP requests made by the crawler.
  properties:
    proxy:
      type: string
      description: Proxy for all crawler requests.
    timeout:
      type: number
      default: 30000
      description: Timeout in milliseconds for the crawl.
    retries:
      type: number
      default: 3
      description: Maximum number of retries to crawl one URL.
    headers:
      $ref: '#/headers'

waitTime:
  type: object
  description: Timeout for the HTTP request.
  properties:
    min:
      type: number
      default: 0
      description: Minimum waiting time in milliseconds.
    max:
      type: number
      default: 20000
      description: Maximum waiting time in milliseconds.

initialIndexSettings:
  type: object
  description: |
    [Index settings](/specs/search#tag/_model_index_settings).

headers:
  type: object
  description: Headers to add to all requests.
  properties:
    Accept-Language:
      type: string
      description: Preferred natural language and locale.
      example: 'fr-FR'
    Authorization:
      type: string
      description: Basic authentication header.
      example: 'Bearer Aerehdf=='
    Cookie:
      type: string
      description: Cookie. The header will be replaced by the cookie retrieved when logging in.
      example: session=1234

login:
  description: Authorization method and credentials for crawling protected content.
  oneOf:
    - $ref: '#/fetchRequest'
    - $ref: '#/browserRequest'
    - $ref: '#/oauthRequest'

fetchRequest:
  type: object
  description: Information for making a HTTP request for authorization.
  properties:
    url:
      type: string
      description: URL with your login form.
      example: https://example.com/login
    requestOptions:
      $ref: '#/loginRequestOptions'
  required:
    - url

browserRequest:
  type: object
  description: |
    Information for using a web browser for authorization.
    The browser loads a login page and enters the provided credentials.
  properties:
    url:
      type: string
      description: |
        URL of your login page.

        The crawler looks for an input matching the selector `input[type=text]` or `input[type=email]` for the username and `input[type=password]` for the password.
      example: https://example.com/login
    username:
      type: string
      description: Username for signing in.
      example: crawler
    password:
      type: string
      description: Password for signing in.
      example: s3cr3t
    waitTime:
      $ref: '#/waitTime'
  required:
    - url
    - username
    - password

oauthRequest:
  type: object
  description: |
    Authorization information for using the [OAuth 2.0 client credentials](https://datatracker.ietf.org/doc/html/rfc6749#section-4.4) authorization grant.
    OAuth authorization is supported for [Azure Active Directory version 1](https://learn.microsoft.com/en-us/previous-versions/azure/active-directory/azuread-dev/v1-oauth2-client-creds-grant-flow) as provider.
  properties:
    accessTokenRequest:
      $ref: '#/accessTokenRequest'
  required:
    - accessTokenRequest

loginRequestOptions:
  type: object
  description: Options for the HTTP request for logging in.
  properties:
    method:
      type: string
      description: HTTP method for sending the request.
      default: GET
      example: POST
    headers:
      $ref: '#/headers'
    body:
      type: string
      description: Form content.
      example: 'id=user&password=s3cr3t'
    timeout:
      type: number
      description: Timeout for the request.

accessTokenRequest:
  type: object
  description: |
    Parameters required to make the [access token request](https://datatracker.ietf.org/doc/html/rfc6749#section-4.4.2).
  properties:
    url:
      type: string
      description: URL for the access token endpoint.
    grantType:
      $ref: '#/grantType'
    clientId:
      type: string
      description: |
        [Client identifier](https://datatracker.ietf.org/doc/html/rfc6749#section-2.2).
    clientSecret:
      type: string
      description: Client secret.
    scope:
      type: string
      description: |
        [Access token scope](https://datatracker.ietf.org/doc/html/rfc6749#section-3.3).
    extraParameters:
      $ref: '#/extraParameters'
  required:
    - url
    - grantType
    - clientId
    - clientSecret

grantType:
  type: string
  description: OAuth 2.0 grant type.
  enum:
    - client_credentials

extraParameters:
  type: object
  description: Extra parameters for the authorization request.
  properties:
    resource:
      type: string
      description: |
        App ID URI of the receiving web service.

        For more information, see [Azure Active Directory](https://learn.microsoft.com/en-us/previous-versions/azure/active-directory/azuread-dev/v1-oauth2-client-creds-grant-flow#first-case-access-token-request-with-a-shared-secret).

safetyChecks:
  type: object
  description: Checks to ensure the crawl was successful.
  properties:
    beforeIndexPublishing:
      $ref: '#/beforeIndexPublishing'

beforeIndexPublishing:
  type: object
  description: These checks are triggered after the crawl finishes but before the records are added to the Algolia index.
  properties:
    maxLostRecordsPercentage:
      type: number
      description: |
        Maximum difference in percent between the numbers of records between crawls.

        If the current crawl results in fewer than `1 - maxLostPercentage` records compared to the previous crawl,
        the current crawling task is stopped with a `SafeReindexingError`.
        The crawler will be blocked until you cancel the blocking task.
      minimum: 1
      maximum: 100
      default: 10
    maxFailedUrls:
      type: number
      description: |
        Stops the crawler if a specified number of pages fail to crawl.
        If undefined, the crawler won't stop if it encounters such errors.

schedule:
  type: string
  description: |
    Schedule for running the crawl, expressed in [Later.js](https://bunkat.github.io/later/) syntax.
    If omitted, you must start crawls manually.

    - The interval between two scheduled crawls must be at least 24 hours.
    - Times are in UTC.
    - Minutes must be explicit: `at 3:00 pm` not `at 3 pm`.
    - Everyday is `every 1 day`.
    - Midnight is `at 12:00 pm`.
    - If you omit the time, a crawl might start any time after midnight UTC.
  example: every weekday at 12:00 pm
