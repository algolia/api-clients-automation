Action:
  type: object
  description: |
    How to process crawled URLs.

    Each action defines:

    - The targeted subset of URLs it processes.
    - What information to extract from the web pages.
    - The Algolia indices where the extracted records will be stored.

    If a single web page matches several actions,
    one record is generated for each action.
  properties:
    autoGenerateObjectIDs:
      type: boolean
      description: Whether to generate an `objectID` for records that don't have one.
      default: true
    cache:
      $ref: '#/cache'
    discoveryPatterns:
      type: array
      description: |
        Which _intermediary_ web pages the crawler should visit.
        Use `discoveryPatterns` to define pages that should be visited _just_ for their links to other pages,
        _not_ their content.
        It functions similarly to the `pathsToMatch` action but without record extraction.
      items:
        $ref: '#/urlPattern'
    fileTypesToMatch:
      type: array
      description: |
        File types for crawling non-HTML documents.
      maxItems: 100
      items:
        $ref: '#/fileTypes'
      default:
        - html
      example:
        - html
        - pdf
    hostnameAliases:
      $ref: '#/hostnameAliases'
    indexName:
      type: string
      maxLength: 256
      description: |
        Reference to the index used to store the action's extracted records.
        `indexName` is combined with the prefix you specified in `indexPrefix`.
      example: algolia_website
    name:
      type: string
      description: Unique identifier for the action. This option is required if `schedule` is set.
    pathAliases:
      $ref: '#/pathAliases'
    pathsToMatch:
      type: array
      description: |
        URLs to which this action should apply.

        Uses [micromatch](https://github.com/micromatch/micromatch) for negation, wildcards, and more.
      minItems: 1
      maxItems: 100
      items:
        $ref: '#/urlPattern'
    recordExtractor:
      title: recordExtractor
      type: object
      description: |
        Function for extracting information from a crawled page and transforming it into Algolia records for indexing.

        The Crawler has an [editor](https://www.algolia.com/doc/tools/crawler/getting-started/crawler-configuration/#the-editor) with autocomplete and validation to help you update the `recordExtractor`.
        For details, see the [`recordExtractor` documentation](https://www.algolia.com/doc/tools/crawler/apis/configuration/actions/#parameter-param-recordextractor).
      properties:
        __type:
          $ref: '#/configurationRecordExtractorType'
        source:
          type: string
          description: |
            A JavaScript function (as a string) that returns one or more Algolia records for each crawled page.
    schedule:
      type: string
      description: |
        How often to perform a complete crawl for this action.
        
        For mopre information, consult the [`schedule` parameter documentation](https://www.algolia.com/doc/tools/crawler/apis/configuration/schedule/).
    selectorsToMatch:
      type: array
      description: |
        DOM selectors for nodes that must be present on the page to be processed.
        If the page doesn't match any of the selectors, it's ignored.
      maxItems: 100
      items:
        type: string
        description: |
          Prefix a selector with `!` to ignore matching pages. 
      example:
        - .products
        - '!.featured'
  required:
    - indexName
    - recordExtractor

configurationRecordExtractorType:
  type: string
  enum:
    - function

ActionSchedule:
  description: |
    Schedule for running a crawl with this action.
    For more information, see the `schedule` parameter of the parent crawler configuration object.
  allOf:
    - $ref: './configuration.yml#/Configuration'

fileTypes:
  type: string
  description: |
    For more information, see [Extract data from non-HTML documents](https://www.algolia.com/doc/tools/crawler/extracting-data/non-html-documents/).
  enum:
    - doc
    - email
    - html
    - odp
    - ods
    - odt
    - pdf
    - ppt
    - xls

urlPattern:
  type: string
  description: |
    Use [micromatch](https://github.com/micromatch/micromatch) for negation, wildcards, and more.
  example: https://www.algolia.com/**

hostnameAliases:
  type: object
  example:
    'dev.example.com': 'example.com'
  description: "Key-value pairs to replace matching hostnames found in a sitemap,\non a page, in canonical links, or redirects.\n\n\nDuring a crawl, this action maps one hostname to another whenever the crawler encounters specific URLs.\nThis helps with links to staging environments (like `dev.example.com`) or external hosting services (such as YouTube).\n\n\nFor example, with this `hostnameAliases` mapping:\n\n    {\n    hostnameAliases: {\n        'dev.example.com': 'example.com'\n    }\n    }\n\n1. The crawler encounters `https://dev.example.com/solutions/voice-search/`.\n\n1. `hostnameAliases` transforms the URL to `https://example.com/solutions/voice-search/`.\n\n1. The crawler follows the transformed URL (not the original).\n\n\n**`hostnameAliases` only changes URLs, not page text. In the preceding example, if the extracted text contains the string `dev.example.com`, it remains unchanged.**\n\n\nThe crawler can discover URLs in places such as:\n\n\n- Crawled pages\n\n- Sitemaps\n\n- [Canonical URLs](https://www.algolia.com/doc/tools/crawler/getting-started/crawler-configuration/#canonical-urls-and-crawler-behavior)\n\n- Redirects. \n\n\nHowever, `hostnameAliases` doesn't transform URLs you explicitly set in the `startUrls` or `sitemaps` parameters,\nnor does it affect the `pathsToMatch` action or other configuration elements.\n"
  additionalProperties:
    type: string
    description: Hostname that should be added in the records.
    x-additionalPropertiesName: hostname

pathAliases:
  type: object
  example:
    'example.com':
      '/foo': '/bar'
  description: |
    Key-value pairs to replace matching paths with new values.

    It doesn't replace:

    - URLs in the `startUrls`, `sitemaps`, `pathsToMatch`, and other settings.
    - Paths found in extracted text.

    The crawl continues from the _transformed_ URLs.
    

    For example, if you create a mapping for `{ "dev.example.com": { '/foo': '/bar' } }` and the crawler encounters `https://dev.example.com/foo/hello/`,
    itâ€™s transformed to `https://dev.example.com/bar/hello/`.
    

    > Compare with the `hostnameAliases` action.

  additionalProperties:
    type: object
    description: Hostname for which matching paths should be replaced.
    x-additionalPropertiesName: hostname
    additionalProperties:
      type: string
      description: Key-value pair of a path that should be replaced.
      x-additionalPropertiesName: path

cache:
  type: object
  description: |
    Whether the crawler should cache crawled pages.

    For more information, see [Partial crawls with caching](https://www.algolia.com/doc/tools/crawler/getting-started/crawler-configuration/#partial-crawls-with-caching).
  properties:
    enabled:
      type: boolean
      default: true
      description: Whether the crawler cache is active.
