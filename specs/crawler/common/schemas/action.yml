Action:
  type: object
  description: |
    How to process crawled URLs.

    Each action defines:

    - The targeted subset of URLs it processes.
    - What information to extract from the web pages.
    - The Algolia indices where the extracted records will be stored.

    If a single web page matches several actions,
    one record is generated for each action.
  properties:
    autoGenerateObjectIDs:
      type: boolean
      description: Whether to generate an `objectID` for records that don't have one.
      default: true
    cache:
      $ref: '#/cache'
    discoveryPatterns:
      type: array
      description: |
        Indicates _intermediary_ pages that the crawler should visit.

        For more information, see the [`discoveryPatterns` documentation](https://www.algolia.com/doc/tools/crawler/apis/discoverypatterns/).
      items:
        $ref: '#/urlPattern'
    fileTypesToMatch:
      type: array
      description: |
        File types for crawling non-HTML documents.

        For more information, see [Extract data from non-HTML documents](https://www.algolia.com/doc/tools/crawler/extracting-data/non-html-documents/).
      maxItems: 100
      items:
        $ref: '#/fileTypes'
      default:
        - html
      example:
        - html
        - pdf
    hostnameAliases:
      $ref: '#/hostnameAliases'
    indexName:
      type: string
      maxLength: 256
      description: |
        Reference to the index used to store the action's extracted records.
        `indexName` is combined with the prefix you specified in `indexPrefix`.
      example: algolia_website
    name:
      type: string
      description: Unique identifier for the action. This option is required if `schedule` is set.
    pathAliases:
      $ref: '#/pathAliases'
    pathsToMatch:
      type: array
      description: |
        URLs to which this action should apply.

        Uses [micromatch](https://github.com/micromatch/micromatch) for negation, wildcards, and more.
      minItems: 1
      maxItems: 100
      items:
        $ref: '#/urlPattern'
    recordExtractor:
      title: recordExtractor
      type: object
      description: |
        Function for extracting information from a crawled page and transforming it into Algolia records for indexing.
        The Crawler has an [editor](https://www.algolia.com/doc/tools/crawler/getting-started/crawler-configuration/#the-editor) with autocomplete and validation to help you update the `recordExtractor` property.

        For details, consult the [`recordExtractor` documentation](https://www.algolia.com/doc/tools/crawler/apis/recordextractor/).
      properties:
        __type:
          $ref: '#/configurationRecordExtractorType'
        source:
          type: string
          description: |
            A JavaScript function (as a string) that returns one or more Algolia records for each crawled page.
    selectorsToMatch:
      type: array
      description: |
        DOM selectors for nodes that must be present on the page to be processed.
        If the page doesn't match any of the selectors, it's ignored.
      maxItems: 100
      items:
        type: string
        description: DOM selector. Negation is supported. This lets you ignore pages that match the selector.
      example:
        - .products
        - '!.featured'
  required:
    - indexName
    - recordExtractor

configurationRecordExtractorType:
  type: string
  enum:
    - function

ActionSchedule:
  description: |
    Schedule for running a crawl with this action.
    For more information, see the `schedule` parameter of the parent crawler configuration object.
  allOf:
    - $ref: './configuration.yml#/Configuration'

fileTypes:
  type: string
  description: |
    Supported file type for indexing non-HTML documents.
    
    For more information, see [Extract data from non-HTML documents](https://www.algolia.com/doc/tools/crawler/extracting-data/non-html-documents/).
  enum:
    - doc
    - email
    - html
    - odp
    - ods
    - odt
    - pdf
    - ppt
    - xls

urlPattern:
  type: string
  description: |
    Pattern for matching URLs.

    Uses [micromatch](https://github.com/micromatch/micromatch) for negation, wildcards, and more.
  example: https://www.algolia.com/**

hostnameAliases:
  type: object
  example:
    'dev.example.com': 'example.com'
  description: |
    Key-value pairs to replace matching hostnames found in a sitemap,
    on a page, in canonical links, or redirects.

    For more information, see the [`hostnameAliases` documentation](https://www.algolia.com/doc/tools/crawler/apis/hostnamealiases/).
  additionalProperties:
    type: string
    description: Hostname that should be added in the records.
    x-additionalPropertiesName: hostname

pathAliases:
  type: object
  example:
    'example.com':
      '/foo': '/bar'
  description: |
    Key-value pairs to replace matching paths with new values.
    
    It doesn't replace:
      
    - URLs in the `startUrls`, `sitemaps`, `pathsToMatch`, and other settings.
    - Paths found in extracted text.

    The crawl continues from the _transformed_ URLs.
  additionalProperties:
    type: object
    description: Hostname for which matching paths should be replaced.
    x-additionalPropertiesName: hostname
    additionalProperties:
      type: string
      description: Key-value pair of a path that should be replaced.
      x-additionalPropertiesName: path

cache:
  type: object
  description: |
    Whether the crawler should cache crawled pages.

    For more information, see the [`cache` documentation](https://www.algolia.com/doc/tools/crawler/apis/cache/).
  properties:
    enabled:
      type: boolean
      default: true
      description: Whether the crawler cache is active.
