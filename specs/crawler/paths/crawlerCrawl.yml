post:
  operationId: crawlUrls
  summary: Crawl URLs
  description: |
    Crawls the specified URLs, extracts records from them, and adds them to the index.
    If a crawl is running (the crawler's `reindexing` property is `true`),
    the records are added to a temporary index.

    This operation is rate-limited to 500 requests every 24 hours.
  tags:
    - actions
  x-acl: []
  parameters:
    - $ref: '../common/parameters.yml#/CrawlerIdParameter'
  requestBody:
    content:
      application/json:
        schema:
          title: crawlUrls
          type: object
          properties:
            urls:
              type: array
              description: URLs to crawl.
              items:
                type: string
              example: ['https://www.algolia.com/products/crawler/']
            save:
              type: boolean
              description: |
                Whether the specified URLs should be added to the `extraURLs` property of the crawler configuration.
                If unspecified, the URLs are added to the `extraUrls` field only if they haven't been indexed during the last reindex.
          required:
            - urls
  responses:
    '200':
      $ref: '../common/schemas/responses.yml#/ActionAcknowledged'
    '400':
      $ref: '../../common/responses/InvalidRequest.yml'
    '401':
      $ref: '../common/schemas/responses.yml#/MissingAuthorization'
    '403':
      $ref: '../common/schemas/responses.yml#/NoRightsOnCrawler'
