# Helper: Wait for a task to be published (completed) for a given `index_name` and `task_id`.
#
# @param index_name [String] the `index_name` where the operation was performed. (required)
# @param task_id [Integer] the `task_id` returned in the method response. (required)
# @param max_retries [Integer] the maximum number of retries. (optional, default to 50)
# @param timeout [Proc] the function to decide how long to wait between retries. (optional)
# @param request_options [Hash] the requestOptions to send along with the query, they will be forwarded to the `get_task` method.
# @return [Http::Response] the last get_task response
def wait_for_task(index_name, task_id, max_retries = 50, timeout = -> (retry_count) { [retry_count * 200, 5000].min }, request_options = {})
  retries = 0
  while retries < max_retries
    res = get_task(index_name, task_id, request_options)
    if res.status == 'published'
      return res
    end
    retries += 1
    sleep(timeout.call(retries) / 1000.0)
  end
  raise ApiError, "The maximum number of retries exceeded. (#{max_retries})"
end

# Helper: Wait for an application-level task to be published (completed) for a given `task_id`.
#
# @param task_id [Integer] the `task_id` returned in the method response. (required)
# @param max_retries [Integer] the maximum number of retries. (optional, default to 50)
# @param timeout [Proc] the function to decide how long to wait between retries. (optional)
# @param request_options [Hash] the requestOptions to send along with the query, they will be forwarded to the `get_task` method.
# @return [Http::Response] the last get_task response
def wait_for_app_task(task_id, max_retries = 50, timeout = -> (retry_count) { [retry_count * 200, 5000].min }, request_options = {})
  retries = 0
  while retries < max_retries
    res = get_app_task(task_id, request_options)
    if res.status == 'published'
      return res
    end
    retries += 1
    sleep(timeout.call(retries) / 1000.0)
  end
  raise ApiError, "The maximum number of retries exceeded. (#{max_retries})"
end

# Helper: Wait for an API key to be added, updated or deleted based on a given `operation`.
#
# @param key [String] the `key` that has been added, deleted or updated.
# @param operation [String] the `operation` that was done on a `key`.
# @param api_key [Hash] necessary to know if an `update` operation has been processed, compare fields of the response with it.
# @param max_retries [Integer] the maximum number of retries.
# @param timeout [Proc] the function to decide how long to wait between retries.
# @param request_options [Hash] the requestOptions to send along with the query, they will be forwarded to the `getApikey` method and merged with the transporter requestOptions.
# @return [Http::Response] the last get_api_key response
def wait_for_api_key(key, operation, api_key = {}, max_retries = 50, timeout = -> (retry_count) { [retry_count * 200, 5000].min }, request_options = {})
  retries = 0
  if operation == 'update'
    raise ArgumentError, '`api_key` is required when waiting for an `update` operation.' if api_key.nil?
    while retries < max_retries
      updated_key = get_api_key(key, request_options)
      updated_key_hash = updated_key.to_hash
      equals = true
      api_key.to_hash.each do |k, v|
        equals &&= updated_key_hash[k] == v
      end

      return updated_key if equals

      retries += 1
      sleep(timeout.call(retries) / 1000.0)
    end
  
    raise ApiError, "The maximum number of retries exceeded. (#{max_retries})"
  end

  while retries < max_retries
    begin
      res = get_api_key(key, request_options)
      return res if operation == "add"
    rescue AlgoliaHttpError => e
      if e.code == 404
        return nil if operation == "delete"
      else
        raise e
      end
    end

    retries += 1
    sleep(timeout.call(retries) / 1000.0)
  end

  raise ApiError, "The maximum number of retries exceeded. (#{max_retries})"
end

# Helper: Iterate on the `browse` method of the client to allow aggregating objects of an index.
#
# @param index_name [String] the `index_name` to browse. (required)
# @param browse_params [BrowseParamsObject] the `browse_params` to send along with the query, they will be forwarded to the `browse` method.
# @param request_options [Hash] the requestOptions to send along with the query, they will be forwarded to the `browse` method.
# @param block [Proc] the block to execute on each object of the index.
def browse_objects(index_name, browse_params = Search::BrowseParamsObject.new, request_options = {}, &block)
  browse_params[:hits_per_page] = browse_params[:hits_per_page] || 1000

  hits = []
  loop do
    res = browse(index_name, browse_params, request_options)
    if block_given?
      res.hits.each do |hit|
        block.call(hit)
      end
    else
      hits.concat(res.hits)
    end
    browse_params.cursor = res.cursor
    break if browse_params.cursor.nil?
  end

  hits unless block_given?
end

# Helper: Iterate on the `searchRules` method of the client to allow aggregating rules of an index.
#
# @param index_name [String] the `index_name` to browse rules from. (required)
# @param search_rules_params [SearchRulesParams] the parameters to send along with the query, they will be forwarded to the `searchRules` method.
# @param request_options [Hash] the requestOptions to send along with the query, they will be forwarded to the `searchRules` method.
# @param block [Proc] the block to execute on each rule of the index.
def browse_rules(index_name, search_rules_params = Search::SearchRulesParams.new(hits_per_page: 1000, page: 0), request_options = {}, &block)
  rules = []
  loop do
    res = search_rules(index_name, search_rules_params, request_options)
    if block_given?
      res.hits.each do |rule|
        block.call(rule)
      end
    else
      rules.concat(res.hits)
    end
    search_rules_params.page += 1
    break if res.hits.length < search_rules_params.hits_per_page
  end

  rules unless block_given?
end

# Helper: Iterate on the `searchSynonyms` method of the client to allow aggregating synonyms of an index.
#
# @param index_name [String] the `index_name` to browse synonyms from. (required)
# @param search_synonyms_params [SearchSynonymsParams] the parameters to send along with the query, they will be forwarded to the `searchSynonyms` method.
# @param request_options [Hash] the requestOptions to send along with the query, they will be forwarded to the `searchSynonyms` method.
# @param block [Proc] the block to execute on each synonym of the index.
def browse_synonyms(index_name, search_synonyms_params = Search::SearchSynonymsParams.new(hits_per_page: 1000, page: 0), request_options = {}, &block)
  synonyms = []
  loop do
    res = search_synonyms(index_name, search_synonyms_params, request_options)
    if block_given?
      res.hits.each do |synonym|
        block.call(synonym)
      end
    else
      synonyms.concat(res.hits)
    end
    search_synonyms_params.page += 1
    break if res.hits.length < search_synonyms_params.hits_per_page
  end

  synonyms unless block_given?
end

# Helper: Generates a secured API key based on the given `parent_api_key` and given `restrictions`.
#
# @param parent_api_key [String] Parent API key used the generate the secured key
# @param restrictions [SecuredApiKeyRestrictions] Restrictions to apply on the secured key
#
# @return [String]
#
def generate_secured_api_key(parent_api_key, restrictions = {})
  restrictions = restrictions.to_hash
  if restrictions.key?(:searchParams)
    # merge searchParams with the root of the restrictions

    restrictions.merge!(restrictions[:searchParams])
    restrictions.delete(:searchParams)
  end

  url_encoded_restrictions = Algolia::Transport.stringify_query_params(restrictions).sort.to_h.map do |key, value|
    "#{key}=#{value}"
  end.join('&')

  hmac = OpenSSL::HMAC.hexdigest(OpenSSL::Digest.new('sha256'), parent_api_key, url_encoded_restrictions)
  Base64.encode64("#{hmac}#{url_encoded_restrictions}").gsub("\n", '')
end

# Helper: Retrieves the remaining validity of the previous generated `secured_api_key`, the `validUntil` parameter must have been provided.
#
# @param secured_api_key [String]
#
# @return [Integer]
#
def get_secured_api_key_remaining_validity(secured_api_key)
  now         = Time.now.to_i
  decoded_key = Base64.decode64(secured_api_key)
  regex       = 'validUntil=(\d+)'
  matches     = decoded_key.match(regex)

  if matches.nil?
    raise AlgoliaError, 'The SecuredApiKey doesn\'t have a validUntil parameter.'
  end

  valid_until = matches[1].to_i

  valid_until - now
end

# Helper: Saves the given array of objects in the given index. The `chunked_batch` helper is used under the hood, which creates a `batch` requests with at most 1000 objects in it.
#
# @param index_name [String]: The `index_name` to save `objects` in.
# @param objects [Array]: The array of `objects` to store in the given Algolia `indexName`.
# @param wait_for_tasks [Boolean]: Whether or not we should wait until every `batch` tasks has been processed, this operation may slow the total execution time of this method but is more reliable.
# @param request_options: The request options to send along with the query, they will be merged with the transporter base parameters (headers, query params, timeouts, etc.). (optional)
#
# @return [BatchResponse]
#
def save_objects(index_name, objects, wait_for_tasks = false, request_options = {})
  chunked_batch(
    index_name,
    objects,
    Search::Action::ADD_OBJECT,
    wait_for_tasks,
    1000,
    request_options
  )
end

# Helper: Deletes every records for the given objectIDs. The `chunked_batch` helper is used under the hood, which creates a `batch` requests with at most 1000 objectIDs in it.
#
# @param index_name [String]: The `index_name` to delete `object_ids` from.
# @param object_ids [Array]: The object_ids to delete.
# @param wait_for_tasks [Boolean]: Whether or not we should wait until every `batch` tasks has been processed, this operation may slow the total execution time of this method but is more reliable.
# @param request_options: The request options to send along with the query, they will be merged with the transporter base parameters (headers, query params, timeouts, etc.). (optional)
#
# @return [BatchResponse]
#
def delete_objects(index_name, object_ids, wait_for_tasks = false, request_options = {})
  chunked_batch(
    index_name,
    object_ids.map { |id| { "objectID" => id } },
    Search::Action::DELETE_OBJECT,
    wait_for_tasks,
    1000,
    request_options
  )
end

# Helper: Replaces object content of all the given objects according to their respective `object_id` field. The `chunkedBatch` helper is used under the hood, which creates a `batch` requests with at most 1000 objects in it.
#
# @param index_name [String]: The `index_name` to delete `object_ids` from.
# @param objects [Array]: The objects to partially update.
# @param create_if_not_exists [Boolean]: To be provided if non-existing objects are passed, otherwise, the call will fail.
# @param wait_for_tasks [Boolean] Whether or not we should wait until every `batch` tasks has been processed, this operation may slow the total execution time of this method but is more reliable.
# @param request_options: The request options to send along with the query, they will be merged with the transporter base parameters (headers, query params, timeouts, etc.). (optional)
#
# @return [BatchResponse]
#
def partial_update_objects(index_name, objects, create_if_not_exists, wait_for_tasks = false, request_options = {})
  chunked_batch(
    index_name,
    objects,
    create_if_not_exists ? Search::Action::PARTIAL_UPDATE_OBJECT : Search::Action::PARTIAL_UPDATE_OBJECT_NO_CREATE,
    wait_for_tasks,
    1000,
    request_options
  )
end

# Helper: Chunks the given `objects` list in subset of 1000 elements max in order to make it fit in `batch` requests.
#
# @param index_name [String] the `index_name` where the operation will be performed.
# @param objects [Array] The array of `objects` to store in the given Algolia `index_name`.
# @param action [Action] The `batch` `action` to perform on the given array of `objects`, defaults to `addObject`.
# @param wait_for_tasks [Boolean] Whether or not we should wait until every `batch` tasks has been processed, this operation may slow the total execution time of this method but is more reliable.
# @param batch_size [int] The size of the chunk of `objects`. The number of `batch` calls will be equal to `length(objects) / batchSize`. Defaults to 1000.
# @param request_options: The request options to send along with the query, they will be merged with the transporter base parameters (headers, query params, timeouts, etc.). (optional)
#
# @return [Array<BatchResponse>]
#
def chunked_batch(index_name, objects, action = Action::ADD_OBJECT, wait_for_tasks = false, batch_size = 1000, request_options = {})
  responses = []
  objects.each_slice(batch_size) do |chunk|
    requests = chunk.map do |object|
      Search::BatchRequest.new(action: action, body: object)
    end

    responses.append(batch(index_name, Search::BatchWriteParams.new(requests: requests), request_options))
  end

  if wait_for_tasks
    responses.each do |response|
      wait_for_task(index_name, response.task_id)
    end
  end

  responses
end

# Helper: Replaces all objects (records) in the given `index_name` with the given `objects`. A temporary index is created during this process in order to backup your data.
#
# @param index_name [String] The `index_name` to replace `objects` in.
# @param objects [Array] The array of `objects` to store in the given Algolia `index_name`.
# @param batch_size [int] The size of the chunk of `objects`. The number of `batch` calls will be equal to `length(objects) / batchSize`. Defaults to 1000.
# @param request_options: The request options to send along with the query, they will be merged with the transporter base parameters (headers, query params, timeouts, etc.). (optional)
#
# @return [Array<ReplaceAllObjectsResponse>]
def replace_all_objects(index_name, objects, batch_size = 1000, request_options = {})
  tmp_index_name = index_name + '_tmp_' + rand(10_000_000).to_s

  copy_operation_response = operation_index(
    index_name,
    Search::OperationIndexParams.new(
      operation: Search::OperationType::COPY,
      destination: tmp_index_name,
      scope: [
        Search::ScopeType::SETTINGS,
        Search::ScopeType::RULES,
        Search::ScopeType::SYNONYMS
      ]
    ),
    request_options
  )

  batch_responses = chunked_batch(
    tmp_index_name,
    objects,
    Search::Action::ADD_OBJECT,
    true,
    batch_size,
    request_options
  )

  wait_for_task(tmp_index_name, copy_operation_response.task_id)

  copy_operation_response = operation_index(
    index_name,
    Search::OperationIndexParams.new(
      operation: Search::OperationType::COPY,
      destination: tmp_index_name,
      scope: [
        Search::ScopeType::SETTINGS,
        Search::ScopeType::RULES,
        Search::ScopeType::SYNONYMS
      ]
    ),
    request_options
  )

  wait_for_task(tmp_index_name, copy_operation_response.task_id)

  move_operation_response = operation_index(
    tmp_index_name,
    Search::OperationIndexParams.new(
      operation: Search::OperationType::MOVE,
      destination: index_name
    ),
    request_options
  )

  wait_for_task(tmp_index_name, move_operation_response.task_id)

  Search::ReplaceAllObjectsResponse.new(
    copy_operation_response: copy_operation_response,
    batch_responses: batch_responses,
    move_operation_response: move_operation_response
  )
end

def index_exists?(index_name)
  begin
    get_settings(index_name)
  rescue AlgoliaHttpError => e
    return false if e.code == 404

    raise e
  end

  true
end