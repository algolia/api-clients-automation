    async def wait_for_task(
        self,
        index_name: str,
        task_id: int,
        timeout: RetryTimeout = RetryTimeout(),
        max_retries: int = 50,
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> GetTaskResponse:
        """
        Helper: Wait for a task to be published (completed) for a given `indexName` and `taskID`.
        """
        self._retry_count = 0

        async def _func(_: GetTaskResponse) -> GetTaskResponse:
            return await self.get_task(index_name, task_id, request_options)

        def _aggregator(_: GetTaskResponse) -> None:
            self._retry_count += 1

        return await create_iterable(
            func=_func,
            aggregator=_aggregator,
            validate=lambda _resp: _resp.status == "published",
            timeout=lambda: timeout(self._retry_count),
            error_validate=lambda x: self._retry_count >= max_retries,
            error_message=lambda: f"The maximum number of retries exceeded. (${self._retry_count}/${max_retries})",
        )

    async def wait_for_app_task(
        self,
        task_id: int,
        timeout: RetryTimeout = RetryTimeout(),
        max_retries: int = 50,
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> GetTaskResponse:
        """
        Helper: Wait for an application-level task to complete for a given `taskID`.
        """
        self._retry_count = 0

        async def _func(_: GetTaskResponse) -> GetTaskResponse:
            return await self.get_app_task(task_id, request_options)

        def _aggregator(_: GetTaskResponse) -> None:
            self._retry_count += 1

        return await create_iterable(
            func=_func,
            aggregator=_aggregator,
            validate=lambda _resp: _resp.status == "published",
            timeout=lambda: timeout(self._retry_count),
            error_validate=lambda x: self._retry_count >= max_retries,
            error_message=lambda: f"The maximum number of retries exceeded. (${self._retry_count}/${max_retries})",
        )

    async def wait_for_api_key(
        self,
        operation: str,
        key: str,
        api_key: Optional[ApiKey] = None,
        max_retries: int = 50,
        timeout: RetryTimeout = RetryTimeout(),
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> GetApiKeyResponse:
        """
        Helper: Wait for an API key to be added, updated or deleted based on a given `operation`.
        """
        self._retry_count = 0

        if operation == "update" and api_key is None:
            raise ValueError(
                "`apiKey` is required when waiting for an `update` operation."
            )

        async def _func(_prev: GetApiKeyResponse) -> GetApiKeyResponse:
            try:
                return await self.get_api_key(key=key, request_options=request_options)
            except RequestException as e:
                if e.status_code == 404 and (operation == "delete" or operation == "add"):
                    return None
                raise e

        def _aggregator(_: GetApiKeyResponse) -> None:
            self._retry_count += 1

        def _validate(_resp: GetApiKeyResponse) -> bool:
            if operation == "update":
                for field in api_key:
                    if isinstance(api_key[field], list) and isinstance(_resp[field], list):
                        if len(api_key[field]) != len(_resp[field]) or any(
                            v != _resp[field][i] for i, v in enumerate(api_key[field])
                        ):
                            return False
                    elif api_key[field] != _resp[field]:
                        return False
                return True
            elif operation == "add":
                return _resp is not None
            return _resp is None

        return await create_iterable(
            func=_func,
            validate=_validate,
            aggregator=_aggregator,
            timeout=lambda: timeout(self._retry_count),
            error_validate=lambda _: self._retry_count >= max_retries,
            error_message=lambda _: f"The maximum number of retries exceeded. (${self._retry_count}/${max_retries})",
        )

    async def browse_objects(
        self,
        index_name: str,
        aggregator: Optional[Callable[[BrowseResponse], None]],
        browse_params: Optional[BrowseParams] = None,
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> BrowseResponse:
        """
        Helper: Iterate on the `browse` method of the client to allow aggregating objects of an index.
        """
        async def _func(_prev: BrowseResponse) -> BrowseResponse:
            if _prev is not None and _prev.cursor is not None:
                browse_params.cursor = _prev.cursor
            return await self.browse(
                index_name=index_name,
                browse_params=browse_params,
                request_options=request_options,
            )

        return await create_iterable(
            func=_func,
            validate=lambda _resp: _resp.cursor is None,
            aggregator=aggregator,
        )

    async def browse_rules(
        self,
        index_name: str,
        aggregator: Optional[Callable[[SearchRulesResponse], None]],
        search_rules_params: Optional[SearchRulesParams] = SearchRulesParams(hits_per_page=1000),
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> SearchRulesResponse:
        """
        Helper: Iterate on the `search_rules` method of the client to allow aggregating rules of an index.
        """
        if search_rules_params is not None:
            search_rules_params.hits_per_page = 1000

        async def _func(_prev: SearchRulesResponse) -> SearchRulesResponse:
            if _prev is not None:
                search_rules_params.page = _prev.page + 1
            return await self.search_rules(
                index_name=index_name,
                search_rules_params=search_rules_params,
                request_options=request_options,
            )
        return await create_iterable(
            func=_func,
            validate=lambda _resp: _resp.nb_hits < search_rules_params.hits_per_page,
            aggregator=aggregator,
        )

    async def browse_synonyms(
        self,
        index_name: str,
        aggregator: Callable[[SearchSynonymsResponse], None],
        search_synonyms_params: Optional[SearchSynonymsParams] = SearchSynonymsParams(),
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> SearchSynonymsResponse:
        """
        Helper: Iterate on the `search_synonyms` method of the client to allow aggregating synonyms of an index.
        """
        if search_synonyms_params.page is None:
            search_synonyms_params.page = 0
        search_synonyms_params.hits_per_page = 1000

        async def _func(_prev: SearchRulesResponse) -> SearchRulesResponse:
            resp = await self.search_synonyms(
                index_name=index_name,
                search_synonyms_params=search_synonyms_params,
                request_options=request_options,
            )
            search_synonyms_params.page += 1
            return resp
        return await create_iterable(
            func=_func,
            validate=lambda _resp: _resp.nb_hits < search_synonyms_params.hits_per_page,
            aggregator=aggregator,
        )

    def generate_secured_api_key(
        self,
        parent_api_key: str,
        restrictions: Optional[SecuredApiKeyRestrictions] = SecuredApiKeyRestrictions(),
    ) -> str:
        """
        Helper: Generates a secured API key based on the given `parent_api_key` and given `restrictions`.
        """
        if not isinstance(restrictions, SecuredApiKeyRestrictions):
            restrictions = SecuredApiKeyRestrictions.from_dict(restrictions)

        restrictions = restrictions.to_dict()
        if "searchParams" in restrictions:
            restrictions = {**restrictions, **restrictions["searchParams"]}
            del restrictions["searchParams"]

        query_parameters = QueryParametersSerializer(dict(sorted(restrictions.items()))).encoded()

        secured_key = hmac.new(
            parent_api_key.encode(encoding="utf-8"),
            query_parameters.encode(encoding="utf-8"),
            hashlib.sha256,
        ).hexdigest()

        base64encoded = base64.b64encode(
            ("{}{}".format(secured_key, query_parameters)).encode(encoding="utf-8")
        )

        return str(base64encoded.decode("utf-8"))

    def get_secured_api_key_remaining_validity(self, secured_api_key: str) -> int:
        """
        Helper: Retrieves the remaining validity of the previous generated `secured_api_key`, the `validUntil` parameter must have been provided.
        """
        validity = search(
            r'validUntil=(\d+)', str(base64.b64decode(secured_api_key))
        )

        if validity is None:
            raise ValidUntilNotFoundException("validUntil not found in api key.")

        return int(validity.group(1)) - int(round(time()))

    def create_temporary_name(self, index_name: str) -> str:
        """
        Helper: Creates a temporary index name from the given `index_name`.
        """
        return "{}_tmp_{}".format(index_name, randint(1000000, 9999999))

    async def save_objects(
        self,
        index_name: str,
        objects: List[Dict[str, Any]],
    ) -> List[BatchResponse]:
        """
        Helper: Saves the given array of objects in the given index. The `chunked_batch` helper is used under the hood, which creates a `batch` requests with at most 1000 objects in it.
        """
        return await self.chunked_batch(index_name=index_name, objects=objects, action=Action.ADDOBJECT)

    async def delete_objects(
        self,
        index_name: str,
        object_ids: List[str],
    ) -> List[BatchResponse]:
        """
        Helper: Deletes every records for the given objectIDs. The `chunked_batch` helper is used under the hood, which creates a `batch` requests with at most 1000 objectIDs in it.
        """
        return await self.chunked_batch(index_name=index_name, objects=[{"objectID": id} for id in object_ids], action=Action.DELETEOBJECT)

    async def partial_update_objects(
        self,
        index_name: str,
        objects: List[Dict[str, Any]],
        create_if_not_exists: Optional[bool] = False,
    ) -> List[BatchResponse]:
        """
        Helper: Replaces object content of all the given objects according to their respective `objectID` field. The `chunked_batch` helper is used under the hood, which creates a `batch` requests with at most 1000 objects in it.
        """
        return await self.chunked_batch(index_name=index_name, objects=objects, action=Action.PARTIALUPDATEOBJECT and create_if_not_exists or Action.PARTIALUPDATEOBJECTNOCREATE)

    async def chunked_batch(
        self,
        index_name: str,
        objects: List[Dict[str, Any]],
        action: Action = Action.ADDOBJECT,
        wait_for_tasks: bool = False,
        batch_size: int = 1000,
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> List[BatchResponse]:
        """
        Helper: Chunks the given `objects` list in subset of 1000 elements max in order to make it fit in `batch` requests.
        """
        requests: List[BatchRequest] = []
        responses: List[BatchResponse] = []
        for i, obj in enumerate(objects):
            requests.append(BatchRequest(action=action, body=obj))
            if len(requests) == batch_size or i == len(objects) - 1:
                responses.append(
                    await self.batch(
                        index_name=index_name,
                        batch_write_params=BatchWriteParams(requests=requests),
                        request_options=request_options,
                    )
                )
                requests = []
        if wait_for_tasks:
            for response in responses:
                await self.wait_for_task(
                    index_name=index_name, task_id=response.task_id
                )
        return responses

    async def replace_all_objects(
        self,
        index_name: str,
        objects: List[Dict[str, Any]],
        batch_size: int = 1000,
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> List[ApiResponse[str]]:
        """
        Helper: Replaces all objects (records) in the given `index_name` with the given `objects`. A temporary index is created during this process in order to backup your data.

        See https://api-clients-automation.netlify.app/docs/contributing/add-new-api-client#5-helpers for implementation details.
        """
        tmp_index_name = self.create_temporary_name(index_name)

        async def _copy() -> UpdatedAtResponse:
            return await self.operation_index(
                index_name=index_name,
                operation_index_params=OperationIndexParams(
                    operation="copy",
                    destination=tmp_index_name,
                    scope=[
                        ScopeType("settings"),
                        ScopeType("synonyms"),
                        ScopeType("rules"),
                    ],
                ),
                request_options=request_options,
            )

        copy_operation_response = await _copy()

        batch_responses = await self.chunked_batch(
            index_name=tmp_index_name,
            objects=objects,
            wait_for_tasks=True,
            batch_size=batch_size,
            request_options=request_options,
        )

        await self.wait_for_task(
            index_name=tmp_index_name, task_id=copy_operation_response.task_id
        )

        copy_operation_response = await _copy()
        await self.wait_for_task(
            index_name=tmp_index_name, task_id=copy_operation_response.task_id
        )

        move_operation_response = await self.operation_index(
            index_name=tmp_index_name,
            operation_index_params=OperationIndexParams(
                operation="move",
                destination=index_name,
            ),
            request_options=request_options,
        )
        await self.wait_for_task(
            index_name=tmp_index_name, task_id=move_operation_response.task_id
        )

        return ReplaceAllObjectsResponse(
            copy_operation_response=copy_operation_response,
            batch_responses=batch_responses,
            move_operation_response=move_operation_response,
        )