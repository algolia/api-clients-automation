    {{^isSyncClient}}async {{/isSyncClient}}def wait_for_task(
        self,
        index_name: str,
        task_id: int,
        timeout: RetryTimeout = RetryTimeout(),
        max_retries: int = 50,
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> GetTaskResponse:
        """
        Helper: Wait for a task to be published (completed) for a given `indexName` and `taskID`.
        """
        _retry_count = 0

        {{^isSyncClient}}async {{/isSyncClient}}def _func(_: Optional[GetTaskResponse]) -> GetTaskResponse:
            return {{^isSyncClient}}await {{/isSyncClient}}self.get_task(index_name, task_id, request_options)

        def _aggregator(_: GetTaskResponse) -> None:
            nonlocal _retry_count
            _retry_count += 1

        return {{^isSyncClient}}await {{/isSyncClient}}create_iterable{{#isSyncClient}}_sync{{/isSyncClient}}(
            func=_func,
            aggregator=_aggregator,
            validate=lambda _resp: _resp.status == "published",
            timeout=lambda: timeout(_retry_count),
            error_validate=lambda _: _retry_count >= max_retries,
            error_message=lambda _: f"The maximum number of retries exceeded. (${_retry_count}/${max_retries})",
        )

    {{^isSyncClient}}async {{/isSyncClient}}def wait_for_app_task(
        self,
        task_id: int,
        timeout: RetryTimeout = RetryTimeout(),
        max_retries: int = 50,
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> GetTaskResponse:
        """
        Helper: Wait for an application-level task to complete for a given `taskID`.
        """
        _retry_count = 0

        {{^isSyncClient}}async {{/isSyncClient}}def _func(_: Optional[GetTaskResponse]) -> GetTaskResponse:
            return {{^isSyncClient}}await {{/isSyncClient}}self.get_app_task(task_id, request_options)

        def _aggregator(_: GetTaskResponse) -> None:
            nonlocal _retry_count
            _retry_count += 1

        return {{^isSyncClient}}await {{/isSyncClient}}create_iterable{{#isSyncClient}}_sync{{/isSyncClient}}(
            func=_func,
            aggregator=_aggregator,
            validate=lambda _resp: _resp.status == "published",
            timeout=lambda: timeout(_retry_count),
            error_validate=lambda _: _retry_count >= max_retries,
            error_message=lambda _: f"The maximum number of retries exceeded. (${_retry_count}/${max_retries})",
        )

    {{^isSyncClient}}async {{/isSyncClient}}def wait_for_api_key(
        self,
        key: str,
        operation: str,
        api_key: Optional[Union[ApiKey, dict[str, Any]]] = None,
        max_retries: int = 50,
        timeout: RetryTimeout = RetryTimeout(),
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> GetApiKeyResponse | None:
        """
        Helper: Wait for an API key to be added, updated or deleted based on a given `operation`.
        """
        _retry_count = 0

        if operation == "update" and api_key is None:
            raise ValueError(
                "`apiKey` is required when waiting for an `update` operation."
            )

        {{^isSyncClient}}async {{/isSyncClient}}def _func(_prev: Optional[GetApiKeyResponse]) -> GetApiKeyResponse:
            try:
                return {{^isSyncClient}}await {{/isSyncClient}}self.get_api_key(key=key, request_options=request_options)
            except RequestException as e:
                if e.status_code == 404 and (operation == "delete" or operation == "add"):
                    return None # pyright: ignore
                raise e

        def _aggregator(_: GetApiKeyResponse | None) -> None:
            nonlocal _retry_count
            _retry_count += 1

        def _validate(_resp: GetApiKeyResponse | None) -> bool:
            if operation == "update":
                if _resp is None:
                    return False
                resp_dict = _resp.to_dict()
                api_key_dict = api_key.to_dict() if isinstance(api_key, ApiKey) else api_key
                if api_key_dict is None:
                    return False
                for field in api_key_dict:
                    if isinstance(api_key_dict[field], list) and isinstance(
                      resp_dict[field], list
                    ):
                        if len(api_key_dict[field]) != len(resp_dict[field]) or any(
                            v != resp_dict[field][i] for i, v in enumerate(api_key_dict[field])
                        ):
                            return False
                    elif api_key_dict[field] != resp_dict[field]:
                        return False
                return True
            elif operation == "add":
                return _resp is not None
            return _resp is None

        return {{^isSyncClient}}await {{/isSyncClient}}create_iterable{{#isSyncClient}}_sync{{/isSyncClient}}(
            func=_func,
            validate=_validate,
            aggregator=_aggregator,
            timeout=lambda: timeout(_retry_count),
            error_validate=lambda _: _retry_count >= max_retries,
            error_message=lambda _: f"The maximum number of retries exceeded. (${_retry_count}/${max_retries})",
        )

    {{^isSyncClient}}async {{/isSyncClient}}def browse_objects(
        self,
        index_name: str,
        aggregator: Callable[[BrowseResponse], None],
        browse_params: BrowseParamsObject = BrowseParamsObject(),
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> BrowseResponse:
        """
        Helper: Iterate on the `browse` method of the client to allow aggregating objects of an index.
        """
        {{^isSyncClient}}async {{/isSyncClient}}def _func(_prev: Optional[BrowseResponse]) -> BrowseResponse:
            if _prev is not None and _prev.cursor is not None:
                browse_params.cursor = _prev.cursor
            return {{^isSyncClient}}await {{/isSyncClient}}self.browse(
                index_name=index_name,
                browse_params=BrowseParams(browse_params),
                request_options=request_options,
            )

        return {{^isSyncClient}}await {{/isSyncClient}}create_iterable{{#isSyncClient}}_sync{{/isSyncClient}}(
            func=_func,
            validate=lambda _resp: _resp.cursor is None,
            aggregator=aggregator,
        )

    {{^isSyncClient}}async {{/isSyncClient}}def browse_rules(
        self,
        index_name: str,
        aggregator: Callable[[SearchRulesResponse], None],
        search_rules_params: SearchRulesParams = SearchRulesParams(hits_per_page=1000),
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> SearchRulesResponse:
        """
        Helper: Iterate on the `search_rules` method of the client to allow aggregating rules of an index.
        """
        if search_rules_params.hits_per_page is None:
            search_rules_params.hits_per_page = 1000
        hits_per_page = search_rules_params.hits_per_page

        {{^isSyncClient}}async {{/isSyncClient}}def _func(_prev: Optional[SearchRulesResponse]) -> SearchRulesResponse:
            if _prev is not None:
                search_rules_params.page = _prev.page + 1
            return {{^isSyncClient}}await {{/isSyncClient}}self.search_rules(
                index_name=index_name,
                search_rules_params=search_rules_params,
                request_options=request_options,
            )
        return {{^isSyncClient}}await {{/isSyncClient}}create_iterable{{#isSyncClient}}_sync{{/isSyncClient}}(
            func=_func,
            validate=lambda _resp: _resp.nb_hits < hits_per_page,
            aggregator=aggregator,
        )

    {{^isSyncClient}}async {{/isSyncClient}}def browse_synonyms(
        self,
        index_name: str,
        aggregator: Callable[[SearchSynonymsResponse], None],
        search_synonyms_params: SearchSynonymsParams = SearchSynonymsParams(hits_per_page=1000),
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> SearchSynonymsResponse:
        """
        Helper: Iterate on the `search_synonyms` method of the client to allow aggregating synonyms of an index.
        """
        hits_per_page = 1000
        page = search_synonyms_params.page or 0
        search_synonyms_params.hits_per_page = hits_per_page

        {{^isSyncClient}}async {{/isSyncClient}}def _func(_prev: Optional[SearchSynonymsResponse]) -> SearchSynonymsResponse:
            nonlocal page
            resp = {{^isSyncClient}}await {{/isSyncClient}}self.search_synonyms(
                index_name=index_name,
                search_synonyms_params=search_synonyms_params,
                request_options=request_options,
            )
            page += 1
            search_synonyms_params.page = page
            return resp
        return {{^isSyncClient}}await {{/isSyncClient}}create_iterable{{#isSyncClient}}_sync{{/isSyncClient}}(
            func=_func,
            validate=lambda _resp: _resp.nb_hits < hits_per_page,
            aggregator=aggregator,
        )

    {{^isSyncClient}}async {{/isSyncClient}}def generate_secured_api_key(
        self,
        parent_api_key: str,
        restrictions: Optional[Union[dict, SecuredApiKeyRestrictions]] = SecuredApiKeyRestrictions(),
    ) -> str:
        """
        Helper: Generates a secured API key based on the given `parent_api_key` and given `restrictions`.
        """
        restrictions_dict = {}
        if isinstance(restrictions, SecuredApiKeyRestrictions):
          restrictions_dict = restrictions.to_dict()
        elif isinstance(restrictions, dict):
          restrictions_dict = restrictions

        if "searchParams" in restrictions_dict:
            restrictions_dict = {**restrictions_dict, **restrictions_dict["searchParams"]}
            del restrictions_dict["searchParams"]

        query_parameters = QueryParametersSerializer(dict(sorted(restrictions_dict.items()))).encoded()

        secured_key = hmac.new(
            parent_api_key.encode(encoding="utf-8"),
            query_parameters.encode(encoding="utf-8"),
            hashlib.sha256,
        ).hexdigest()

        base64encoded = base64.b64encode(
            ("{}{}".format(secured_key, query_parameters)).encode(encoding="utf-8")
        )

        return str(base64encoded.decode("utf-8"))

    def get_secured_api_key_remaining_validity(self, secured_api_key: str) -> int:
        """
        Helper: Retrieves the remaining validity of the previous generated `secured_api_key`, the `validUntil` parameter must have been provided.
        """
        validity = search(
            r'validUntil=(\d+)', str(base64.b64decode(secured_api_key))
        )

        if validity is None:
            raise ValidUntilNotFoundException("validUntil not found in api key.")

        return int(validity.group(1)) - int(round(time()))

    def create_temporary_name(self, index_name: str) -> str:
        """
        Helper: Creates a temporary index name from the given `index_name`.
        """
        return "{}_tmp_{}".format(index_name, randint(1000000, 9999999))

    {{^isSyncClient}}async {{/isSyncClient}}def save_objects(
        self,
        index_name: str,
        objects: List[Dict[str, Any]],
        wait_for_tasks: bool = False,
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> List[BatchResponse]:
        """
        Helper: Saves the given array of objects in the given index. The `chunked_batch` helper is used under the hood, which creates a `batch` requests with at most 1000 objects in it.
        """
        return {{^isSyncClient}}await {{/isSyncClient}}self.chunked_batch(index_name=index_name, objects=objects, action=Action.ADDOBJECT, wait_for_tasks=wait_for_tasks, request_options=request_options)

    {{^isSyncClient}}async {{/isSyncClient}}def delete_objects(
        self,
        index_name: str,
        object_ids: List[str],
        wait_for_tasks: bool = False,
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> List[BatchResponse]:
        """
        Helper: Deletes every records for the given objectIDs. The `chunked_batch` helper is used under the hood, which creates a `batch` requests with at most 1000 objectIDs in it.
        """
        return {{^isSyncClient}}await {{/isSyncClient}}self.chunked_batch(index_name=index_name, objects=[{"objectID": id} for id in object_ids], action=Action.DELETEOBJECT, wait_for_tasks=wait_for_tasks, request_options=request_options)

    {{^isSyncClient}}async {{/isSyncClient}}def partial_update_objects(
        self,
        index_name: str,
        objects: List[Dict[str, Any]],
        create_if_not_exists: bool = False,
        wait_for_tasks: bool = False,
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> List[BatchResponse]:
        """
        Helper: Replaces object content of all the given objects according to their respective `objectID` field. The `chunked_batch` helper is used under the hood, which creates a `batch` requests with at most 1000 objects in it.
        """
        return {{^isSyncClient}}await {{/isSyncClient}}self.chunked_batch(index_name=index_name, objects=objects, action=Action.PARTIALUPDATEOBJECT if create_if_not_exists else Action.PARTIALUPDATEOBJECTNOCREATE, wait_for_tasks=wait_for_tasks, request_options=request_options)

    {{^isSyncClient}}async {{/isSyncClient}}def chunked_batch(
        self,
        index_name: str,
        objects: List[Dict[str, Any]],
        action: Action = Action.ADDOBJECT,
        wait_for_tasks: bool = False,
        batch_size: int = 1000,
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> List[BatchResponse]:
        """
        Helper: Chunks the given `objects` list in subset of 1000 elements max in order to make it fit in `batch` requests.
        """
        requests: List[BatchRequest] = []
        responses: List[BatchResponse] = []
        for i, obj in enumerate(objects):
            requests.append(BatchRequest(action=action, body=obj))
            if len(requests) == batch_size or i == len(objects) - 1:
                responses.append(
                    {{^isSyncClient}}await {{/isSyncClient}}self.batch(
                        index_name=index_name,
                        batch_write_params=BatchWriteParams(requests=requests),
                        request_options=request_options,
                    )
                )
                requests = []
        if wait_for_tasks:
            for response in responses:
                {{^isSyncClient}}await {{/isSyncClient}}self.wait_for_task(
                    index_name=index_name, task_id=response.task_id
                )
        return responses

    {{^isSyncClient}}async {{/isSyncClient}}def replace_all_objects(
        self,
        index_name: str,
        objects: List[Dict[str, Any]],
        batch_size: int = 1000,
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> ReplaceAllObjectsResponse:
        """
        Helper: Replaces all objects (records) in the given `index_name` with the given `objects`. A temporary index is created during this process in order to backup your data.

        See https://api-clients-automation.netlify.app/docs/add-new-api-client#5-helpers for implementation details.
        """
        tmp_index_name = self.create_temporary_name(index_name)

        {{^isSyncClient}}async {{/isSyncClient}}def _copy() -> UpdatedAtResponse:
            return {{^isSyncClient}}await {{/isSyncClient}}self.operation_index(
                index_name=index_name,
                operation_index_params=OperationIndexParams(
                    operation=OperationType.COPY,
                    destination=tmp_index_name,
                    scope=[
                        ScopeType("settings"),
                        ScopeType("rules"),
                        ScopeType("synonyms"),
                    ],
                ),
                request_options=request_options,
            )

        copy_operation_response = {{^isSyncClient}}await {{/isSyncClient}}_copy()

        batch_responses = {{^isSyncClient}}await {{/isSyncClient}}self.chunked_batch(
            index_name=tmp_index_name,
            objects=objects,
            wait_for_tasks=True,
            batch_size=batch_size,
            request_options=request_options,
        )

        {{^isSyncClient}}await {{/isSyncClient}}self.wait_for_task(
            index_name=tmp_index_name, task_id=copy_operation_response.task_id
        )

        copy_operation_response = {{^isSyncClient}}await {{/isSyncClient}}_copy()
        {{^isSyncClient}}await {{/isSyncClient}}self.wait_for_task(
            index_name=tmp_index_name, task_id=copy_operation_response.task_id
        )

        move_operation_response = {{^isSyncClient}}await {{/isSyncClient}}self.operation_index(
            index_name=tmp_index_name,
            operation_index_params=OperationIndexParams(
                operation=OperationType.MOVE,
                destination=index_name,
            ),
            request_options=request_options,
        )
        {{^isSyncClient}}await {{/isSyncClient}}self.wait_for_task(
            index_name=tmp_index_name, task_id=move_operation_response.task_id
        )

        return ReplaceAllObjectsResponse(
            copy_operation_response=copy_operation_response,
            batch_responses=batch_responses,
            move_operation_response=move_operation_response,
        )

    {{^isSyncClient}}async {{/isSyncClient}}def index_exists(self, index_name: str) -> bool:
        """
        Helper: Checks if the given `index_name` exists.
        """
        try:
            {{^isSyncClient}}await {{/isSyncClient}}self.get_settings(index_name)
        except Exception as e:
            if isinstance(e, RequestException) and e.status_code == 404:
                return False
            raise e

        return True