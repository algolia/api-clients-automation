    async def wait_for_task(
        self,
        index_name: str,
        task_id: int,
        timeout: RetryTimeout = RetryTimeout(),
        max_retries: int = 50,
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> GetTaskResponse:
        """
        Helper: Wait for a task to be published (completed) for a given `indexName` and `taskID`.
        """
        self._retry_count = 0

        async def _func(_: GetTaskResponse) -> GetTaskResponse:
            return await self.get_task(index_name, task_id, request_options)

        def _aggregator(_: GetTaskResponse) -> None:
            self._retry_count += 1

        return await create_iterable(
            func=_func,
            aggregator=_aggregator,
            validate=lambda _resp: _resp.status == "published",
            timeout=lambda: timeout(self._retry_count),
            error_validate=lambda x: self._retry_count >= max_retries,
            error_message=lambda: f"The maximum number of retries exceeded. (${self._retry_count}/${max_retries})",
        )

    async def wait_for_api_key(
        self,
        operation: str,
        key: str,
        api_key: Optional[ApiKey] = None,
        max_retries: int = 50,
        timeout: RetryTimeout = RetryTimeout(),
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> GetApiKeyResponse:
        """
        Helper: Wait for an API key to be added, updated or deleted based on a given `operation`.
        """
        self._retry_count = 0

        if operation == "update" and api_key is None:
            raise ValueError(
                "`apiKey` is required when waiting for an `update` operation."
            )

        async def _func(_prev: GetApiKeyResponse) -> GetApiKeyResponse:
            try:
                return await self.get_api_key(key=key, request_options=request_options)
            except RequestException as e:
                if e.status_code == 404 and (operation == "delete" or operation == "add"):
                    return None
                raise e

        def _aggregator(_: GetApiKeyResponse) -> None:
            self._retry_count += 1

        def _validate(_resp: GetApiKeyResponse) -> bool:
            if operation == "update":
                for field in api_key:
                    if isinstance(api_key[field], list) and isinstance(_resp[field], list):
                        if len(api_key[field]) != len(_resp[field]) or any(
                            v != _resp[field][i] for i, v in enumerate(api_key[field])
                        ):
                            return False
                    elif api_key[field] != _resp[field]:
                        return False
                return True
            elif operation == "add":
                return _resp is not None
            return _resp is None

        return await create_iterable(
            func=_func,
            validate=_validate,
            aggregator=_aggregator,
            timeout=lambda: timeout(self._retry_count),
            error_validate=lambda _: self._retry_count >= max_retries,
            error_message=lambda _: f"The maximum number of retries exceeded. (${self._retry_count}/${max_retries})",
        )

    async def browse_objects(
        self,
        index_name: str,
        aggregator: Optional[Callable[[BrowseResponse], None]],
        browse_params: Optional[BrowseParams] = None,
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> BrowseResponse:
        """
        Helper: Iterate on the `browse` method of the client to allow aggregating objects of an index.
        """
        async def _func(_prev: BrowseResponse) -> BrowseResponse:
            if _prev is not None and _prev.cursor is not None:
                browse_params.cursor = _prev.cursor
            return await self.browse(
                index_name=index_name,
                browse_params=browse_params,
                request_options=request_options,
            )

        return await create_iterable(
            func=_func,
            validate=lambda _resp: _resp.cursor is None,
            aggregator=aggregator,
        )

    async def browse_rules(
        self,
        index_name: str,
        aggregator: Optional[Callable[[SearchRulesResponse], None]],
        search_rules_params: Optional[SearchRulesParams] = SearchRulesParams(hits_per_page=1000),
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> SearchRulesResponse:
        """
        Helper: Iterate on the `search_rules` method of the client to allow aggregating rules of an index.
        """
        if search_rules_params is not None:
            search_rules_params.hits_per_page = 1000

        async def _func(_prev: SearchRulesResponse) -> SearchRulesResponse:
            if _prev is not None:
                search_rules_params.page = _prev.page + 1
            return await self.search_rules(
                index_name=index_name,
                search_rules_params=search_rules_params,
                request_options=request_options,
            )
        return await create_iterable(
            func=_func,
            validate=lambda _resp: _resp.nb_hits < search_rules_params.hits_per_page,
            aggregator=aggregator,
        )

    async def browse_synonyms(
        self,
        index_name: str,
        aggregator: Callable[[SearchSynonymsResponse], None],
        search_synonyms_params: Optional[SearchSynonymsParams] = SearchSynonymsParams(),
        request_options: Optional[Union[dict, RequestOptions]] = None,
    ) -> SearchSynonymsResponse:
        """
        Helper: Iterate on the `search_synonyms` method of the client to allow aggregating synonyms of an index.
        """
        if search_synonyms_params.page is None:
            search_synonyms_params.page = 0
        search_synonyms_params.hits_per_page = 1000

        async def _func(_prev: SearchRulesResponse) -> SearchRulesResponse:
            resp = await self.search_synonyms(
                index_name=index_name,
                search_synonyms_params=search_synonyms_params,
                request_options=request_options,
            )
            search_synonyms_params.page += 1
            return resp
        return await create_iterable(
            func=_func,
            validate=lambda _resp: _resp.nb_hits < search_synonyms_params.hits_per_page,
            aggregator=aggregator,
        )

    def generate_secured_api_key(
        self,
        parent_api_key: str,
        restrictions: Optional[SecuredApiKeyRestrictions] = SecuredApiKeyRestrictions(),
    ) -> str:
        """
        Helper: Generates a secured API key based on the given `parent_api_key` and given `restrictions`.
        """
        query_parameters = dumps(
            QueryParametersSerializer(
                restrictions.to_dict()
                if isinstance(restrictions, SecuredApiKeyRestrictions)
                else restrictions
            ).query_parameters
        )

        secured_key = hmac.new(
            parent_api_key.encode(encoding="utf-8"),
            query_parameters.encode(encoding="utf-8"),
            hashlib.sha256,
        ).hexdigest()

        base64encoded = base64.b64encode(
            ("{}{}".format(secured_key, query_parameters)).encode(encoding="utf-8")
        )

        return str(base64encoded.decode("utf-8"))

    def get_secured_api_key_remaining_validity(self, secured_api_key: str) -> int:
        """
        Helper: Retrieves the remaining validity of the previous generated `secured_api_key`, the `ValidUntil` parameter must have been provided.
        """
        validity = search(
            r'"valid_until": "(\d+)"', str(base64.b64decode(secured_api_key))
        )

        if validity is None:
            raise ValidUntilNotFoundException("valid_until not found in api key.")

        return int(validity.group(1)) - int(round(time()))

    def create_temporary_name(self, index_name: str) -> str:
        """
        Helper: Creates a temporary index name from the given `index_name`.
        """
        return "{}_tmp_{}".format(index_name, "".join(choice(ascii_letters) for i in range(10)))

    async def replace_all_objects(self, index_name: str, objects: Union[List[Dict[str, Any]], Iterator[Dict[str, Any]]], safe: Optional[bool] = False, request_options: Optional[Union[dict, RequestOptions]] = None) -> List[ApiResponse[str]]:
        """
        Helper: Replaces all objects (records) in the given `index_name` with the given `objects`. A temporary index is created during this process in order to backup your data.
        """
        tmp_index_name = self.create_temporary_name(index_name)
        responses: List[ApiResponse[str]] = []
        copy_resp = await self.operation_index(
                index_name=index_name,
                operation_index_params=OperationIndexParams(
                    operation="copy",
                    destination=tmp_index_name,
                    scope=[ScopeType("settings"), ScopeType("synonyms"), ScopeType("rules")]
                ),
                request_options=request_options,
            )

        responses.append(copy_resp)

        if safe:
            await self.wait_for_task(index_name=tmp_index_name, task_id=copy_resp.task_id)

        requests: List[BatchRequest] = []

        for obj in objects:
            requests.append({"action": "addObject", "body": obj})

        save_resp = await self.batch(
                index_name=tmp_index_name,
                batch_write_params=BatchWriteParams(requests=requests),
                request_options=request_options,
        )

        responses.append(save_resp)

        if safe:
            await self.wait_for_task(index_name=tmp_index_name, task_id=save_resp.task_id)

        move_resp = await self.operation_index(
                index_name=tmp_index_name,
                operation_index_params={
                    "operation": "move",
                    "destination": index_name,
                },
                request_options=request_options,
            )

        responses.append(move_resp)

        if safe:
            await self.wait_for_task(index_name=index_name, task_id=move_resp.task_id)

        return responses